[2024-09-18 16:45:35,035][test.py][line:34][INFO] ---------------args---------------
Namespace(data_path='./data/Pymatgen_Wrapped/2', batch_size=128, model_path='/data/ylh/MyExps/MOFV2/checkpoints/AtLSmall/AtLSmall_epoch_28.pth', device='2', mode='test', top_k=5, parallel_model=False, test_name='AtLSmall', num_workers=20, log_name='log/test/test_AtLSmall_2024_09_18_16:45:30.log')
[2024-09-18 16:45:35,036][test.py][line:35][INFO] ---------------model---------------
AtLSmall(
  (embed): Embedding(8501, 15, padding_idx=0)
  (att): TransformerEncoder(
    (positionEnbeding): PositionEmbedding()
    (encoder_layers): ModuleList(
      (0-7): 8 x EncoderLayer(
        (mha): MultiHeadAttention(
          (WQ): Linear(in_features=160, out_features=160, bias=True)
          (WK): Linear(in_features=160, out_features=160, bias=True)
          (WV): Linear(in_features=160, out_features=160, bias=True)
          (scaled_dot_product_attn): ScaledDotProductAttention()
          (linear): Linear(in_features=160, out_features=160, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (layernorm1): LayerNorm((160,), eps=1e-06, elementwise_affine=True)
        (ffn): FeedForwardNetwork(
          (linear1): Linear(in_features=160, out_features=256, bias=True)
          (linear2): Linear(in_features=256, out_features=160, bias=True)
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (dropout2): Dropout(p=0.1, inplace=False)
        (layernorm2): LayerNorm((160,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (cls): Sequential(
    (0): Linear(in_features=160, out_features=200, bias=True)
    (1): Linear(in_features=200, out_features=230, bias=True)
  )
)
[2024-09-18 16:45:35,036][test.py][line:36][INFO] ---------------device---------------
cuda:2
[2024-09-18 16:45:35,036][test.py][line:37][INFO] ---------------lossfn---------------
CrossEntropyLoss()
[2024-09-18 16:48:23,657][test.py][line:78][INFO] ---------------performance---------------
total_num:141939
error:3.089830
total_acc:0.3347846567630768
f1_score:0.15651172399520874
top5_acc:0.7216339707374573
head_acc:0.37383873528670963
medium_acc:0.19518856377974714
tail_add:0.13726373814828002

[2024-09-18 16:48:23,658][test.py][line:89][INFO] ---------------per_class_acc---------------
[0.39611572, 0.3810453, 0.0, 0.2978862, 0.3286322, 0.0, 0.09087154, 0.06818182, 0.42920828, 0.05, 0.23799314, 0.14126807, 0.07652474, 0.3490213, 0.28553095, 0.6818182, 0.0, 0.34964705, 0.6917382, 0.12582782, 0.0, 0.0, 0.048, 0.0, 0.0, 0.05263158, 0.0, 0.0, 0.076717, 0.07042254, 0.06909091, 0.0, 0.31962666, 0.09677419, 0.2, 0.16426513, 0.01724138, 0.045454547, 0.0, 0.05102041, 0.0, 0.15789473, 0.17117615, 0.0, 0.028938906, 0.041666668, 0.13333334, 0.0, 0.0, 0.14814815, 0.03846154, 0.11016949, 0.042857144, 0.015748031, 0.033333335, 0.06925946, 0.02016129, 0.09090909, 0.015151516, 0.33266398, 0.6327141, 0.35849056, 0.18789144, 0.0, 0.13592233, 0.03076923, 0.0, 0.0, 0.07936508, 0.080617495, 0.028169014, 0.088785045, 0.0064102565, 0.08391608, 0.0, 0.08898305, 0.06451613, 0.12589073, 0.02238806, 0.070866145, 0.060344826, 0.203966, 0.125, 0.0, 0.21380846, 0.17443609, 0.21606648, 0.33333334, 0.0, 0.115384616, 0.045454547, 0.33435896, 0.0, 0.094339624, 0.06382979, 0.11891892, 0.0, 0.4117647, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.05263158, 0.04347826, 0.089201875, 0.0, 0.05882353, 0.19424461, 0.22916667, 0.0, 0.0952381, 0.030303031, 0.021276595, 0.0, 0.05357143, 0.1954023, 0.072674416, 0.17948718, 0.38297874, 0.36666667, 0.05357143, 0.0, 0.10989011, 0.14606741, 0.15102041, 0.25714287, 0.0, 0.0, 0.035714287, 0.04, 0.15384616, 0.03508772, 0.10958904, 0.07826087, 0.097222224, 0.192, 0.20879121, 0.19298245, 0.25515464, 0.24870466, 0.25174826, 0.3457792, 0.62167645, 0.33333334, 0.018518519, 0.0, 0.17805383, 0.0, 0.13720317, 0.14457831, 0.0, 0.0, 0.01923077, 0.12432432, 0.1953125, 0.21317829, 0.06896552, 0.27572018, 0.18367347, 0.27272728, 0.375, 0.2579583, 0.07692308, 0.096045196, 0.1037464, 0.025, 0.0, 0.11232877, 0.0, 0.041666668, 0.20151134, 0.0, 0.034013607, 0.08, 0.0, 0.0, 0.30927834, 0.0, 0.0, 0.0, 0.125, 0.36363637, 0.0, 0.0, 0.011363637, 0.3030303, 0.5084746, 0.02631579, 0.26666668, 0.1, 0.4827586, 0.32142857, 0.7123288, 0.53731346, 0.0, 0.12, 0.041666668, 0.2, 0.328125, 0.6993007, 0.2112676, 0.23076923, 0.0, 0.14285715, 0.32258064, 0.44827586, 0.20833333, 0.4848485, 0.2857143, 0.18181819, 0.09090909, 0.5521472, 0.43661973, 0.020833334, 0.4520548, 0.47244096, 0.525641, 0.15789473, 0.1875, 0.63235295, 0.14285715, 0.2840909, 0.41666666, 0.18181819, 0.20338982]
