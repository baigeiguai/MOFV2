[2024-09-16 12:58:12,670][train.py][line:72][INFO] ---------------args---------------
Namespace(data_path='./data/Pymatgen_Wrapped/0', train_name='ConvAttV2', model_path=None, learning_rate=0.001, min_learning_rate=1e-06, start_scheduler_step=0, weight_decay=1e-06, momentum=0.99, batch_size=128, class_num=230, epoch_num=200, model_save_path='/data/ylh/MyExps/MOFV2/checkpoints/ConvAttV2', device='4,5,6,7', scheduler_T=None, num_workers=20, log_name='log/train//train_ConvAttV2_2024_09_16_12:58:08.log')
[2024-09-16 12:58:12,673][train.py][line:73][INFO] ---------------model---------------
DataParallel(
  (module): ConvAttV2(
    (conv): Sequential(
      (0): ResBlock1D(
        (pre): Conv1d(2, 32, kernel_size=(1,), stride=(1,), bias=False)
        (conv): Sequential(
          (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (1): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
      (2): ResBlock1D(
        (pre): Identity()
        (conv): Sequential(
          (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (3): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
      (4): ResBlock1D(
        (pre): Conv1d(32, 64, kernel_size=(1,), stride=(1,), bias=False)
        (conv): Sequential(
          (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (5): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
      (6): ResBlock1D(
        (pre): Identity()
        (conv): Sequential(
          (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (7): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
    )
    (att): TransformerEncoder(
      (positionEnbeding): PositionEmbedding()
      (encoder_layers): ModuleList(
        (0-47): 48 x EncoderLayer(
          (mha): MultiHeadAttention(
            (WQ): Linear(in_features=64, out_features=64, bias=True)
            (WK): Linear(in_features=64, out_features=64, bias=True)
            (WV): Linear(in_features=64, out_features=64, bias=True)
            (scaled_dot_product_attn): ScaledDotProductAttention()
            (linear): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout1): Dropout(p=0, inplace=False)
          (layernorm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (linear1): Linear(in_features=64, out_features=256, bias=True)
            (linear2): Linear(in_features=256, out_features=64, bias=True)
            (relu): LeakyReLU(negative_slope=0.01)
          )
          (dropout2): Dropout(p=0, inplace=False)
          (layernorm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (cls): Sequential(
      (0): Linear(in_features=64, out_features=128, bias=True)
      (1): Linear(in_features=128, out_features=230, bias=True)
    )
  )
)
[2024-09-16 12:58:12,673][train.py][line:74][INFO] ---------------device---------------
cuda:4
[2024-09-16 12:58:12,673][train.py][line:75][INFO] ---------------optimizer---------------
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 1e-06
)
[2024-09-16 12:58:12,673][train.py][line:76][INFO] ---------------lossfn---------------
CrossEntropyLoss()
[2024-09-16 12:58:12,674][train.py][line:77][INFO] ---------------seed---------------
3407
[2024-09-16 12:58:12,692][train.py][line:89][INFO] ---------------epoch 1---------------
lr: [0.001]
[2024-09-16 13:14:56,454][train.py][line:107][INFO] [training]total_num: 142618.0,error: 3.7946355122851396
[2024-09-16 13:20:46,425][train.py][line:150][INFO] [testing]total_number: 142618,error: 3.7788450337133264,total_acc: 0.06608562916517258
[2024-09-16 13:20:47,055][train.py][line:89][INFO] ---------------epoch 2---------------
lr: [0.0009998767596502603]
[2024-09-16 13:30:52,619][train.py][line:107][INFO] [training]total_num: 142618.0,error: 3.776344049685596
[2024-09-16 13:39:08,324][train.py][line:150][INFO] [testing]total_number: 142618,error: 3.773126036484049,total_acc: 0.06493569910526276
[2024-09-16 13:39:08,510][train.py][line:89][INFO] ---------------epoch 3---------------
lr: [0.0009995687100869909]
[2024-09-16 13:56:02,959][train.py][line:107][INFO] [training]total_num: 142618.0,error: 3.77081565911937
[2024-09-16 14:04:14,731][train.py][line:150][INFO] [testing]total_number: 142618,error: 3.7722286644929683,total_acc: 0.06614172458648682
[2024-09-16 14:04:14,904][train.py][line:89][INFO] ---------------epoch 4---------------
lr: [0.0009991375531879113]
[2024-09-16 14:21:11,769][train.py][line:107][INFO] [training]total_num: 142618.0,error: 3.770257774875983
[2024-09-16 14:29:51,823][train.py][line:150][INFO] [testing]total_number: 142618,error: 3.7709574382074553,total_acc: 0.06608562916517258
[2024-09-16 14:29:52,021][train.py][line:89][INFO] ---------------epoch 5---------------
lr: [0.000998583395328906]
[2024-09-16 14:48:02,990][train.py][line:107][INFO] [training]total_num: 142618.0,error: 3.7707735827885167
[2024-09-16 14:56:18,763][train.py][line:150][INFO] [testing]total_number: 142618,error: 3.771467302579634,total_acc: 0.06608562916517258
[2024-09-16 14:56:18,769][train.py][line:89][INFO] ---------------epoch 6---------------
lr: [0.000997906373234505]
[2024-09-16 15:14:30,700][train.py][line:107][INFO] [training]total_num: 142618.0,error: 3.771244264220176
[2024-09-16 15:23:12,893][train.py][line:150][INFO] [testing]total_number: 142618,error: 3.771059155993051,total_acc: 0.06608562916517258
[2024-09-16 15:23:12,901][train.py][line:89][INFO] ---------------epoch 7---------------
lr: [0.0009971066539441438]
[2024-09-16 15:35:13,394][train.py][line:107][INFO] [training]total_num: 142618.0,error: 3.7702411168850727
[2024-09-16 15:41:15,364][train.py][line:150][INFO] [testing]total_number: 142618,error: 3.771175499605519,total_acc: 0.06538445502519608
[2024-09-16 15:41:15,372][train.py][line:89][INFO] ---------------epoch 8---------------
lr: [0.0009961844347709525]
[2024-09-16 15:52:11,781][train.py][line:107][INFO] [training]total_num: 142618.0,error: 3.769572002864455
[2024-09-16 15:57:18,359][train.py][line:150][INFO] [testing]total_number: 142618,error: 3.7711261198810484,total_acc: 0.06614172458648682
[2024-09-16 15:57:18,366][train.py][line:89][INFO] ---------------epoch 9---------------
lr: [0.000995139943253065]
[2024-09-16 16:07:04,340][train.py][line:107][INFO] [training]total_num: 142618.0,error: 3.767972071401398
[2024-09-16 16:13:10,549][train.py][line:150][INFO] [testing]total_number: 142618,error: 3.769054260524692,total_acc: 0.06614172458648682
[2024-09-16 16:13:10,719][train.py][line:89][INFO] ---------------epoch 10---------------
lr: [0.0009939734370974804]
[2024-09-16 16:27:08,433][train.py][line:107][INFO] [training]total_num: 142618.0,error: 3.7675987095989374
[2024-09-16 16:33:49,250][train.py][line:150][INFO] [testing]total_number: 142618,error: 3.7698887541110047,total_acc: 0.06538445502519608
[2024-09-16 16:33:49,256][train.py][line:89][INFO] ---------------epoch 11---------------
lr: [0.0009926852041164747]
[2024-09-16 16:50:54,537][train.py][line:107][INFO] [training]total_num: 142618.0,error: 3.7688961424654104
[2024-09-16 16:59:14,792][train.py][line:150][INFO] [testing]total_number: 142618,error: 3.7706719337571695,total_acc: 0.06608562916517258
[2024-09-16 16:59:14,799][train.py][line:89][INFO] ---------------epoch 12---------------
lr: [0.0009912755621565839]
[2024-09-16 17:16:10,227][train.py][line:107][INFO] [training]total_num: 142618.0,error: 3.7645485227681306
[2024-09-16 17:24:09,137][train.py][line:150][INFO] [testing]total_number: 142618,error: 3.770323951792273,total_acc: 0.06538445502519608
[2024-09-16 17:24:09,144][train.py][line:89][INFO] ---------------epoch 13---------------
lr: [0.0009897448590201828]
[2024-09-16 17:40:58,846][train.py][line:107][INFO] [training]total_num: 142618.0,error: 3.7667977619509627
[2024-09-16 17:49:16,564][train.py][line:150][INFO] [testing]total_number: 142618,error: 3.7688550523878095,total_acc: 0.06588228791952133
[2024-09-16 17:49:16,745][train.py][line:89][INFO] ---------------epoch 14---------------
lr: [0.000988093472379664]
[2024-09-16 18:06:33,012][train.py][line:107][INFO] [training]total_num: 142618.0,error: 3.765599998416579
