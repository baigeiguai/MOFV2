[2024-09-10 13:11:59,214][train.py][line:68][INFO] ---------------args---------------
Namespace(data_path='./data/Pymatgen_Wrapped/0', train_name='ConvAtt_extend', model_path='/data/ylh/MyExps/MOFV2/checkpoints/ConvAtt_extend/ConvAtt_extend_epoch_22.pth', learning_rate=5e-05, min_learning_rate=1e-06, start_scheduler_step=0, weight_decay=1e-06, momentum=0.99, batch_size=32, class_num=230, epoch_num=200, model_save_path='/data/ylh/MyExps/MOFV2/checkpoints/ConvAtt_extend', device='1,3,5,7', scheduler_T=None, num_workers=20, log_name='log/train//train_ConvAtt_extend_2024_09_10_13:11:55.log')
[2024-09-10 13:11:59,218][train.py][line:69][INFO] ---------------model---------------
DataParallel(
  (module): ConvAtt(
    (model): Sequential(
      (0): CAtBlock(
        (conv): ResBlock1D(
          (pre): Conv1d(2, 16, kernel_size=(1,), stride=(1,), bias=False)
          (conv): Sequential(
            (0): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=16, out_features=16, bias=True)
                (WK): Linear(in_features=16, out_features=16, bias=True)
                (WV): Linear(in_features=16, out_features=16, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=16, out_features=16, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((16,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=16, out_features=32, bias=True)
                (linear2): Linear(in_features=32, out_features=16, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((16,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (1): CAtBlock(
        (conv): ResBlock1D(
          (pre): Identity()
          (conv): Sequential(
            (0): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=16, out_features=16, bias=True)
                (WK): Linear(in_features=16, out_features=16, bias=True)
                (WV): Linear(in_features=16, out_features=16, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=16, out_features=16, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((16,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=16, out_features=32, bias=True)
                (linear2): Linear(in_features=32, out_features=16, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((16,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (2): CAtBlock(
        (conv): ResBlock1D(
          (pre): Conv1d(16, 32, kernel_size=(1,), stride=(1,), bias=False)
          (conv): Sequential(
            (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=32, out_features=32, bias=True)
                (WK): Linear(in_features=32, out_features=32, bias=True)
                (WV): Linear(in_features=32, out_features=32, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=32, out_features=32, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=32, out_features=64, bias=True)
                (linear2): Linear(in_features=64, out_features=32, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (3): CAtBlock(
        (conv): ResBlock1D(
          (pre): Identity()
          (conv): Sequential(
            (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=32, out_features=32, bias=True)
                (WK): Linear(in_features=32, out_features=32, bias=True)
                (WV): Linear(in_features=32, out_features=32, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=32, out_features=32, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=32, out_features=64, bias=True)
                (linear2): Linear(in_features=64, out_features=32, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (4): CAtBlock(
        (conv): ResBlock1D(
          (pre): Conv1d(32, 64, kernel_size=(1,), stride=(1,), bias=False)
          (conv): Sequential(
            (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=64, out_features=64, bias=True)
                (WK): Linear(in_features=64, out_features=64, bias=True)
                (WV): Linear(in_features=64, out_features=64, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=64, out_features=64, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=64, out_features=128, bias=True)
                (linear2): Linear(in_features=128, out_features=64, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (5): CAtBlock(
        (conv): ResBlock1D(
          (pre): Identity()
          (conv): Sequential(
            (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=64, out_features=64, bias=True)
                (WK): Linear(in_features=64, out_features=64, bias=True)
                (WV): Linear(in_features=64, out_features=64, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=64, out_features=64, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=64, out_features=128, bias=True)
                (linear2): Linear(in_features=128, out_features=64, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (6): CAtBlock(
        (conv): ResBlock1D(
          (pre): Conv1d(64, 128, kernel_size=(1,), stride=(1,), bias=False)
          (conv): Sequential(
            (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=128, out_features=128, bias=True)
                (WK): Linear(in_features=128, out_features=128, bias=True)
                (WV): Linear(in_features=128, out_features=128, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=128, out_features=128, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=128, out_features=256, bias=True)
                (linear2): Linear(in_features=256, out_features=128, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (7): CAtBlock(
        (conv): ResBlock1D(
          (pre): Identity()
          (conv): Sequential(
            (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=128, out_features=128, bias=True)
                (WK): Linear(in_features=128, out_features=128, bias=True)
                (WV): Linear(in_features=128, out_features=128, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=128, out_features=128, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=128, out_features=256, bias=True)
                (linear2): Linear(in_features=256, out_features=128, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (8): CAtBlock(
        (conv): ResBlock1D(
          (pre): Conv1d(128, 256, kernel_size=(1,), stride=(1,), bias=False)
          (conv): Sequential(
            (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=256, out_features=256, bias=True)
                (WK): Linear(in_features=256, out_features=256, bias=True)
                (WV): Linear(in_features=256, out_features=256, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (9): CAtBlock(
        (conv): ResBlock1D(
          (pre): Identity()
          (conv): Sequential(
            (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=256, out_features=256, bias=True)
                (WK): Linear(in_features=256, out_features=256, bias=True)
                (WV): Linear(in_features=256, out_features=256, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (10): CAtBlock(
        (conv): ResBlock1D(
          (pre): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (conv): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=512, out_features=512, bias=True)
                (WK): Linear(in_features=512, out_features=512, bias=True)
                (WV): Linear(in_features=512, out_features=512, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=512, out_features=512, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (11): CAtBlock(
        (conv): ResBlock1D(
          (pre): Identity()
          (conv): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=512, out_features=512, bias=True)
                (WK): Linear(in_features=512, out_features=512, bias=True)
                (WV): Linear(in_features=512, out_features=512, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=512, out_features=512, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (12): CAtBlock(
        (conv): ResBlock1D(
          (pre): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
          (conv): Sequential(
            (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=1024, out_features=1024, bias=True)
                (WK): Linear(in_features=1024, out_features=1024, bias=True)
                (WV): Linear(in_features=1024, out_features=1024, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (13): CAtBlock(
        (conv): ResBlock1D(
          (pre): Identity()
          (conv): Sequential(
            (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=1024, out_features=1024, bias=True)
                (WK): Linear(in_features=1024, out_features=1024, bias=True)
                (WV): Linear(in_features=1024, out_features=1024, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
    )
    (cls): Linear(in_features=1024, out_features=230, bias=True)
  )
)
[2024-09-10 13:11:59,219][train.py][line:70][INFO] ---------------device---------------
cuda:1
[2024-09-10 13:11:59,219][train.py][line:71][INFO] ---------------optimizer---------------
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 5e-05
    lr: 5e-05
    maximize: False
    weight_decay: 1e-06
)
[2024-09-10 13:11:59,219][train.py][line:72][INFO] ---------------lossfn---------------
CrossEntropyLoss()
[2024-09-10 13:11:59,219][train.py][line:73][INFO] ---------------seed---------------
3407
[2024-09-10 13:11:59,223][train.py][line:85][INFO] ---------------epoch 1---------------
lr: [5e-05]
[2024-09-10 13:30:26,082][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.380699510348954
[2024-09-10 13:38:34,848][train.py][line:146][INFO] [testing]total_number: 142618,error: 1.3871619020418455,total_acc: 0.5786436200141907
[2024-09-10 13:38:35,901][train.py][line:85][INFO] ---------------epoch 2---------------
lr: [4.999395517804079e-05]
[2024-09-10 13:57:02,944][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.3202658043673603
[2024-09-10 14:05:14,274][train.py][line:146][INFO] [testing]total_number: 142618,error: 1.614421199726802,total_acc: 0.5284115672111511
[2024-09-10 14:05:14,281][train.py][line:85][INFO] ---------------epoch 3---------------
lr: [4.997884563990245e-05]
[2024-09-10 14:23:39,967][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.3319795501808966
[2024-09-10 14:31:52,783][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.0933899268187504,total_acc: 0.31918132305145264
[2024-09-10 14:31:52,789][train.py][line:85][INFO] ---------------epoch 4---------------
lr: [4.995769780401166e-05]
[2024-09-10 14:50:20,539][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.3664195384683362
[2024-09-10 14:58:32,557][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.987920850675593,total_acc: 0.10882917791604996
[2024-09-10 14:58:32,564][train.py][line:85][INFO] ---------------epoch 5---------------
lr: [4.99305168880044e-05]
[2024-09-10 15:16:54,950][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.3428383112274191
[2024-09-10 15:25:03,087][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.935805551894677,total_acc: 0.32312190532684326
[2024-09-10 15:25:03,093][train.py][line:85][INFO] ---------------epoch 6---------------
lr: [4.989730959808882e-05]
[2024-09-10 15:43:17,358][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.484147146624583
[2024-09-10 15:51:28,599][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.322130128848083,total_acc: 0.26269474625587463
[2024-09-10 15:51:28,606][train.py][line:85][INFO] ---------------epoch 7---------------
lr: [4.985808412739043e-05]
[2024-09-10 16:10:20,801][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.376663901726298
[2024-09-10 16:18:41,220][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.9827123501715267,total_acc: 0.3901330828666687
[2024-09-10 16:18:41,227][train.py][line:85][INFO] ---------------epoch 8---------------
lr: [4.9812850153930595e-05]
[2024-09-10 16:37:17,626][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.3491951011815593
[2024-09-10 16:45:34,229][train.py][line:146][INFO] [testing]total_number: 142618,error: 1.8408762959489686,total_acc: 0.535304069519043
[2024-09-10 16:45:34,236][train.py][line:85][INFO] ---------------epoch 9---------------
lr: [4.976161883823841e-05]
[2024-09-10 17:04:05,694][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.3380874469742767
[2024-09-10 17:12:34,382][train.py][line:146][INFO] [testing]total_number: 142618,error: 6.69623911888155,total_acc: 0.11491537094116211
[2024-09-10 17:12:34,389][train.py][line:85][INFO] ---------------epoch 10---------------
lr: [4.970440282059713e-05]
[2024-09-10 17:30:55,514][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.6435601802202322
[2024-09-10 17:39:19,147][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.705436668488928,total_acc: 0.3088740408420563
[2024-09-10 17:39:19,154][train.py][line:85][INFO] ---------------epoch 11---------------
lr: [4.964121621792517e-05]
[2024-09-10 17:58:15,400][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.4570857214083808
[2024-09-10 18:06:34,514][train.py][line:146][INFO] [testing]total_number: 142618,error: 4.232920550744593,total_acc: 0.21263796091079712
[2024-09-10 18:06:34,521][train.py][line:85][INFO] ---------------epoch 12---------------
lr: [4.957207462029289e-05]
[2024-09-10 18:25:21,518][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.3538173790553778
[2024-09-10 18:33:45,053][train.py][line:146][INFO] [testing]total_number: 142618,error: 1.3158835618886897,total_acc: 0.5997279286384583
[2024-09-10 18:33:45,644][train.py][line:85][INFO] ---------------epoch 13---------------
lr: [4.9496995087076e-05]
[2024-09-10 18:52:27,493][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.3752149914167688
[2024-09-10 19:00:44,220][train.py][line:146][INFO] [testing]total_number: 142618,error: 1.9182530295197255,total_acc: 0.45648515224456787
[2024-09-10 19:00:44,226][train.py][line:85][INFO] ---------------epoch 14---------------
lr: [4.941599614274625e-05]
[2024-09-10 19:19:31,351][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.3637260814000416
[2024-09-10 19:27:47,902][train.py][line:146][INFO] [testing]total_number: 142618,error: 4.994866809216878,total_acc: 0.13940036296844482
[2024-09-10 19:27:47,909][train.py][line:85][INFO] ---------------epoch 15---------------
lr: [4.932909777230082e-05]
[2024-09-10 19:46:28,830][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.373144191579633
[2024-09-10 19:54:53,687][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.316349927883422,total_acc: 0.2871516942977905
[2024-09-10 19:54:53,694][train.py][line:85][INFO] ---------------epoch 16---------------
lr: [4.923632141633112e-05]
[2024-09-10 20:13:37,840][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.0316099815043924
[2024-09-10 20:22:04,094][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.455673287926395,total_acc: 0.24368593096733093
[2024-09-10 20:22:04,101][train.py][line:85][INFO] ---------------epoch 17---------------
lr: [4.9137689965732694e-05]
[2024-09-10 20:41:45,981][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.7226611826048102
[2024-09-10 20:50:38,184][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.9780133007408045,total_acc: 0.3311363160610199
[2024-09-10 20:50:38,191][train.py][line:85][INFO] ---------------epoch 18---------------
lr: [4.903322775605711e-05]
[2024-09-10 21:10:02,560][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.6841992832533348
[2024-09-10 21:18:47,511][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.9870250071782793,total_acc: 0.34253740310668945
[2024-09-10 21:18:47,519][train.py][line:85][INFO] ---------------epoch 19---------------
lr: [4.892296056150742e-05]
[2024-09-10 21:38:06,514][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.8294446350318985
[2024-09-10 21:46:44,960][train.py][line:146][INFO] [testing]total_number: 142618,error: 6.141831718518742,total_acc: 0.10151594132184982
[2024-09-10 21:46:44,967][train.py][line:85][INFO] ---------------epoch 20---------------
lr: [4.880691558857875e-05]
[2024-09-10 22:06:16,337][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.80606351767443
[2024-09-10 22:14:44,034][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.2843412375578316,total_acc: 0.18962542712688446
[2024-09-10 22:14:44,040][train.py][line:85][INFO] ---------------epoch 21---------------
lr: [4.868512146934541e-05]
[2024-09-10 22:33:51,933][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.9623313533629567
[2024-09-10 22:42:18,389][train.py][line:146][INFO] [testing]total_number: 142618,error: 4.923997741579796,total_acc: 0.31273752450942993
[2024-09-10 22:42:18,396][train.py][line:85][INFO] ---------------epoch 22---------------
lr: [4.8557608254396214e-05]
[2024-09-10 23:03:45,339][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.902084773252835
[2024-09-10 23:12:18,586][train.py][line:146][INFO] [testing]total_number: 142618,error: 1.9897538711013119,total_acc: 0.4597315788269043
[2024-09-10 23:12:18,592][train.py][line:85][INFO] ---------------epoch 23---------------
lr: [4.842440740542e-05]
[2024-09-10 23:31:29,046][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.156352120141188
[2024-09-10 23:40:18,851][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.9650705865046882,total_acc: 0.24467457830905914
[2024-09-10 23:40:18,858][train.py][line:85][INFO] ---------------epoch 24---------------
lr: [4.8285551787442706e-05]
[2024-09-11 00:00:42,454][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.2731825262575165
[2024-09-11 00:09:11,638][train.py][line:146][INFO] [testing]total_number: 142618,error: 6.551287947376141,total_acc: 0.054845813661813736
[2024-09-11 00:09:11,645][train.py][line:85][INFO] ---------------epoch 25---------------
lr: [4.814107566071841e-05]
[2024-09-11 00:27:53,235][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.190172026901903
[2024-09-11 00:36:11,328][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.8286259444063284,total_acc: 0.19342578947544098
[2024-09-11 00:36:11,334][train.py][line:85][INFO] ---------------epoch 26---------------
lr: [4.799101467227611e-05]
[2024-09-11 00:54:57,490][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.3918381971590836
[2024-09-11 01:03:15,336][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.4306651919973365,total_acc: 0.3520102798938751
[2024-09-11 01:03:15,835][train.py][line:85][INFO] ---------------epoch 27---------------
lr: [4.7835405847124095e-05]
[2024-09-11 01:21:53,050][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.353390477544304
[2024-09-11 01:30:13,435][train.py][line:146][INFO] [testing]total_number: 142618,error: 1.9805331110527011,total_acc: 0.435835599899292
[2024-09-11 01:30:13,442][train.py][line:85][INFO] ---------------epoch 28---------------
lr: [4.767428757911467e-05]
[2024-09-11 01:48:58,369][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.286671816541623
[2024-09-11 01:57:21,046][train.py][line:146][INFO] [testing]total_number: 142618,error: 61.691062629116054,total_acc: 0.03852248564362526
[2024-09-11 01:57:21,053][train.py][line:85][INFO] ---------------epoch 29---------------
lr: [4.750769962147082e-05]
[2024-09-11 02:15:55,749][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.3583620369007083
[2024-09-11 02:24:15,769][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.338478542196708,total_acc: 0.25323590636253357
[2024-09-11 02:24:15,776][train.py][line:85][INFO] ---------------epoch 30---------------
lr: [4.733568307697772e-05]
[2024-09-11 02:43:03,093][train.py][line:103][INFO] [training]total_num: 142618.0,error: 11.334649669093256
[2024-09-11 02:51:26,058][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.795540943047479,total_acc: 0.14588621258735657
[2024-09-11 02:51:26,065][train.py][line:85][INFO] ---------------epoch 31---------------
lr: [4.715828038784109e-05]
[2024-09-11 03:10:07,032][train.py][line:103][INFO] [training]total_num: 142618.0,error: 3.2641432049774353
[2024-09-11 03:18:30,513][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.1799120023900036,total_acc: 0.22788147628307343
[2024-09-11 03:18:30,520][train.py][line:85][INFO] ---------------epoch 32---------------
lr: [4.697553532521513e-05]
[2024-09-11 03:36:58,766][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.998837811785573
[2024-09-11 03:45:10,952][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.262133928690119,total_acc: 0.1994558870792389
[2024-09-11 03:45:10,958][train.py][line:85][INFO] ---------------epoch 33---------------
lr: [4.678749297840248e-05]
[2024-09-11 04:03:40,418][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.7399353414346668
[2024-09-11 04:11:50,615][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.558813460892247,total_acc: 0.3146306872367859
[2024-09-11 04:11:50,622][train.py][line:85][INFO] ---------------epoch 34---------------
lr: [4.659419974372925e-05]
[2024-09-11 04:30:16,017][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.461330404017775
[2024-09-11 04:38:30,897][train.py][line:146][INFO] [testing]total_number: 142618,error: 4.645850051100964,total_acc: 0.1508224755525589
[2024-09-11 04:38:30,904][train.py][line:85][INFO] ---------------epoch 35---------------
lr: [4.639570331309698e-05]
[2024-09-11 04:56:55,256][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.2505693994328966
[2024-09-11 05:05:08,751][train.py][line:146][INFO] [testing]total_number: 142618,error: 7.690603818196977,total_acc: 0.0686308890581131
[2024-09-11 05:05:08,758][train.py][line:85][INFO] ---------------epoch 36---------------
lr: [4.6192052662215566e-05]
[2024-09-11 05:23:40,567][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.189095767114752
[2024-09-11 05:31:55,323][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.4154749261916324,total_acc: 0.3544573485851288
[2024-09-11 05:31:55,330][train.py][line:85][INFO] ---------------epoch 37---------------
lr: [4.598329803851896e-05]
[2024-09-11 05:50:18,296][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.45465924072757
[2024-09-11 05:58:36,065][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.360830025994436,total_acc: 0.37353628873825073
[2024-09-11 05:58:36,071][train.py][line:85][INFO] ---------------epoch 38---------------
lr: [4.576949094876734e-05]
[2024-09-11 06:17:08,795][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.2647375130097926
[2024-09-11 06:25:21,153][train.py][line:146][INFO] [testing]total_number: 142618,error: 7.1121406420050555,total_acc: 0.19937175512313843
[2024-09-11 06:25:21,160][train.py][line:85][INFO] ---------------epoch 39---------------
lr: [4.555068414633844e-05]
[2024-09-11 06:43:48,696][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.315643836039796
[2024-09-11 06:52:04,289][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.5235007694491776,total_acc: 0.3471861779689789
[2024-09-11 06:52:04,296][train.py][line:85][INFO] ---------------epoch 40---------------
lr: [4.532693161821143e-05]
[2024-09-11 07:10:30,415][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.0695804572367114
[2024-09-11 07:18:40,152][train.py][line:146][INFO] [testing]total_number: 142618,error: 10.435911889598575,total_acc: 0.3733960688114166
[2024-09-11 07:18:40,158][train.py][line:85][INFO] ---------------epoch 41---------------
lr: [4.509828857164612e-05]
[2024-09-11 07:37:05,751][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.2256987076987076
[2024-09-11 07:45:17,723][train.py][line:146][INFO] [testing]total_number: 142618,error: 80.97110184097802,total_acc: 0.04516260325908661
[2024-09-11 07:45:17,730][train.py][line:85][INFO] ---------------epoch 42---------------
lr: [4.486481142056149e-05]
[2024-09-11 08:03:50,092][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.2821762444564944
[2024-09-11 08:12:08,701][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.8202814103294442,total_acc: 0.3444516062736511
[2024-09-11 08:12:08,707][train.py][line:85][INFO] ---------------epoch 43---------------
lr: [4.4626557771616115e-05]
[2024-09-11 08:30:36,147][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.44252203204619
[2024-09-11 08:38:48,448][train.py][line:146][INFO] [testing]total_number: 142618,error: 5.366866514861156,total_acc: 0.2493864744901657
[2024-09-11 08:38:48,455][train.py][line:85][INFO] ---------------epoch 44---------------
lr: [4.438358640999466e-05]
[2024-09-11 08:57:17,700][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.8262126893072144
[2024-09-11 09:05:44,082][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.5659181589400895,total_acc: 0.33303651213645935
[2024-09-11 09:05:44,091][train.py][line:85][INFO] ---------------epoch 45---------------
lr: [4.413595728490309e-05]
[2024-09-11 09:24:29,507][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.4698728586023
[2024-09-11 09:32:57,216][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.0793132567544563,total_acc: 0.2767743170261383
[2024-09-11 09:32:57,223][train.py][line:85][INFO] ---------------epoch 46---------------
lr: [4.388373149477728e-05]
[2024-09-11 09:51:56,350][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.7852416708798393
[2024-09-11 10:00:19,913][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.1611336379388755,total_acc: 0.22432652115821838
[2024-09-11 10:00:19,920][train.py][line:85][INFO] ---------------epoch 47---------------
lr: [4.3626971272207524e-05]
[2024-09-11 10:19:04,968][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.6346919682664685
[2024-09-11 10:27:52,164][train.py][line:146][INFO] [testing]total_number: 142618,error: 37.89115902263418,total_acc: 0.29472437500953674
[2024-09-11 10:27:52,171][train.py][line:85][INFO] ---------------epoch 48---------------
lr: [4.336573996858355e-05]
[2024-09-11 10:46:42,517][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.7256445212550062
[2024-09-11 10:55:17,118][train.py][line:146][INFO] [testing]total_number: 142618,error: 42.19734260338395,total_acc: 0.18557965755462646
[2024-09-11 10:55:17,125][train.py][line:85][INFO] ---------------epoch 49---------------
lr: [4.310010203846339e-05]
[2024-09-11 11:14:14,693][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.9059712679582685
[2024-09-11 11:22:32,943][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.117660355690773,total_acc: 0.2879370152950287
[2024-09-11 11:22:32,950][train.py][line:85][INFO] ---------------epoch 50---------------
lr: [4.28301230236699e-05]
[2024-09-11 11:41:22,016][train.py][line:103][INFO] [training]total_num: 142618.0,error: 3.074935325798595
[2024-09-11 11:50:21,991][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.608031444919152,total_acc: 0.20833274722099304
[2024-09-11 11:50:21,999][train.py][line:85][INFO] ---------------epoch 51---------------
lr: [4.255586953711928e-05]
[2024-09-11 12:13:33,660][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.8974543438429903
[2024-09-11 12:24:26,290][train.py][line:146][INFO] [testing]total_number: 142618,error: 4.618355128393378,total_acc: 0.19982050359249115
[2024-09-11 12:24:26,820][train.py][line:85][INFO] ---------------epoch 52---------------
lr: [4.2277409246385114e-05]
[2024-09-11 12:47:34,983][train.py][line:103][INFO] [training]total_num: 142618.0,error: 3.26319753130277
[2024-09-11 12:59:29,679][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.4102593450189493,total_acc: 0.2335609793663025
[2024-09-11 12:59:29,687][train.py][line:85][INFO] ---------------epoch 53---------------
lr: [4.199481085700226e-05]
[2024-09-11 13:21:46,882][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.983103956800208
[2024-09-11 13:30:21,567][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.468063078580364,total_acc: 0.26089975237846375
[2024-09-11 13:30:21,578][train.py][line:85][INFO] ---------------epoch 54---------------
lr: [4.170814409551456e-05]
[2024-09-11 13:53:03,275][train.py][line:103][INFO] [training]total_num: 142618.0,error: 3.0162930271745156
[2024-09-11 14:02:59,530][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.2784053523053407,total_acc: 0.20348763465881348
[2024-09-11 14:02:59,539][train.py][line:85][INFO] ---------------epoch 55---------------
lr: [4.1417479692270816e-05]
[2024-09-11 14:22:41,150][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.984105336089288
[2024-09-11 14:31:59,676][train.py][line:146][INFO] [testing]total_number: 142618,error: 27.191006043692216,total_acc: 0.2749162018299103
[2024-09-11 14:31:59,683][train.py][line:85][INFO] ---------------epoch 56---------------
lr: [4.112288936397281e-05]
[2024-09-11 14:51:43,800][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.8590879067244495
[2024-09-11 15:00:00,907][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.6352877032585895,total_acc: 0.3369140028953552
[2024-09-11 15:00:00,914][train.py][line:85][INFO] ---------------epoch 57---------------
lr: [4.082444579598016e-05]
[2024-09-11 15:20:00,514][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.748430443440287
[2024-09-11 15:29:20,747][train.py][line:146][INFO] [testing]total_number: 142618,error: 4.22979964331914,total_acc: 0.11575677990913391
[2024-09-11 15:29:20,755][train.py][line:85][INFO] ---------------epoch 58---------------
lr: [4.0522222624376144e-05]
[2024-09-11 15:50:31,664][train.py][line:103][INFO] [training]total_num: 142618.0,error: 3.0894802059163755
[2024-09-11 16:00:35,550][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.2101720456688208,total_acc: 0.2299429178237915
[2024-09-11 16:00:35,557][train.py][line:85][INFO] ---------------epoch 59---------------
lr: [4.0216294417798784e-05]
[2024-09-11 16:20:39,150][train.py][line:103][INFO] [training]total_num: 142618.0,error: 3.1119444345701552
[2024-09-11 16:29:45,779][train.py][line:146][INFO] [testing]total_number: 142618,error: 12.507428382593458,total_acc: 0.2657518684864044
[2024-09-11 16:29:45,786][train.py][line:85][INFO] ---------------epoch 60---------------
lr: [3.990673665904205e-05]
[2024-09-11 16:49:54,289][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.8443415692630207
[2024-09-11 17:01:31,853][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.5717747641400197,total_acc: 0.3078222870826721
[2024-09-11 17:01:31,861][train.py][line:85][INFO] ---------------epoch 61---------------
lr: [3.959362572643134e-05]
[2024-09-11 17:23:57,684][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.8818611379806285
[2024-09-11 17:32:37,392][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.9674544005411074,total_acc: 0.3177438974380493
[2024-09-11 17:32:37,399][train.py][line:85][INFO] ---------------epoch 62---------------
lr: [3.927703887497805e-05]
[2024-09-11 17:52:03,731][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.8446676001273175
[2024-09-11 18:00:47,090][train.py][line:146][INFO] [testing]total_number: 142618,error: 4.723058927523834,total_acc: 0.2603949010372162
[2024-09-11 18:00:47,098][train.py][line:85][INFO] ---------------epoch 63---------------
lr: [3.8957054217317864e-05]
[2024-09-11 18:19:53,789][train.py][line:103][INFO] [training]total_num: 142618.0,error: 3.162528864799007
[2024-09-11 18:28:32,636][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.7524755633432805,total_acc: 0.15658612549304962
[2024-09-11 18:28:32,643][train.py][line:85][INFO] ---------------epoch 64---------------
lr: [3.863375070443728e-05]
[2024-09-11 18:47:31,843][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.954062781052991
[2024-09-11 18:56:10,688][train.py][line:146][INFO] [testing]total_number: 142618,error: 5.497381132162814,total_acc: 0.2851533591747284
[2024-09-11 18:56:10,695][train.py][line:85][INFO] ---------------epoch 65---------------
lr: [3.830720810619355e-05]
[2024-09-11 19:15:15,649][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.6954345520198557
[2024-09-11 19:23:57,850][train.py][line:146][INFO] [testing]total_number: 142618,error: 17.90882574945795,total_acc: 0.3313957452774048
[2024-09-11 19:23:57,859][train.py][line:85][INFO] ---------------epoch 66---------------
lr: [3.7977506991632276e-05]
[2024-09-11 19:42:52,606][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.59143849184543
[2024-09-11 19:51:29,888][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.7605245352912973,total_acc: 0.3151705861091614
[2024-09-11 19:51:29,896][train.py][line:85][INFO] ---------------epoch 67---------------
lr: [3.764472870910794e-05]
[2024-09-11 20:10:32,442][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.6271567878757325
[2024-09-11 20:23:10,167][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.933155625898351,total_acc: 0.2848728895187378
[2024-09-11 20:23:10,174][train.py][line:85][INFO] ---------------epoch 68---------------
lr: [3.730895536621221e-05]
[2024-09-11 20:46:36,189][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.554507936147379
[2024-09-11 20:58:31,261][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.7317959136166023,total_acc: 0.3208010196685791
[2024-09-11 20:58:31,269][train.py][line:85][INFO] ---------------epoch 69---------------
lr: [3.697026980951472e-05]
[2024-09-11 21:21:03,975][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.5129764899878517
[2024-09-11 21:32:46,490][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.236535761171558,total_acc: 0.3166220188140869
[2024-09-11 21:32:46,497][train.py][line:85][INFO] ---------------epoch 70---------------
lr: [3.6628755604121754e-05]
[2024-09-11 21:53:50,463][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.529900491718323
[2024-09-11 22:03:56,640][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.671058149107041,total_acc: 0.3465060591697693
[2024-09-11 22:03:56,650][train.py][line:85][INFO] ---------------epoch 71---------------
lr: [3.628449701305732e-05]
[2024-09-11 22:28:34,084][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.6331432437437408
[2024-09-11 22:39:46,387][train.py][line:146][INFO] [testing]total_number: 142618,error: 5.210427318128847,total_acc: 0.3290117681026459
[2024-09-11 22:39:46,395][train.py][line:85][INFO] ---------------epoch 72---------------
lr: [3.593757897647231e-05]
[2024-09-11 23:04:25,103][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.777104970775411
[2024-09-11 23:17:31,412][train.py][line:146][INFO] [testing]total_number: 142618,error: 21.216824901173407,total_acc: 0.320660799741745
[2024-09-11 23:17:31,421][train.py][line:85][INFO] ---------------epoch 73---------------
lr: [3.558808709068643e-05]
[2024-09-11 23:36:27,831][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.829615140794426
[2024-09-11 23:45:02,871][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.0304737236337425,total_acc: 0.27880772948265076
[2024-09-11 23:45:02,878][train.py][line:85][INFO] ---------------epoch 74---------------
lr: [3.523610758706812e-05]
[2024-09-12 00:03:55,159][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.6588215508555
[2024-09-12 00:12:31,172][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.447758893305469,total_acc: 0.3698831796646118
[2024-09-12 00:12:31,179][train.py][line:85][INFO] ---------------epoch 75---------------
lr: [3.488172731075802e-05]
[2024-09-12 00:31:24,739][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.643964884715909
[2024-09-12 00:40:09,994][train.py][line:146][INFO] [testing]total_number: 142618,error: 6.334070588792524,total_acc: 0.11578482389450073
[2024-09-12 00:40:10,001][train.py][line:85][INFO] ---------------epoch 76---------------
lr: [3.4525033699240785e-05]
[2024-09-12 00:59:24,729][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.850365989123835
[2024-09-12 01:08:13,369][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.5934047772145186,total_acc: 0.343855619430542
[2024-09-12 01:08:13,818][train.py][line:85][INFO] ---------------epoch 77---------------
lr: [3.416611476077072e-05]
[2024-09-12 01:27:22,510][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.6380379270481806
[2024-09-12 01:36:03,447][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.9220832731828468,total_acc: 0.3304281234741211
[2024-09-12 01:36:03,453][train.py][line:85][INFO] ---------------epoch 78---------------
lr: [3.380505905265675e-05]
[2024-09-12 01:55:06,712][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.7563905952812098
[2024-09-12 02:03:57,874][train.py][line:146][INFO] [testing]total_number: 142618,error: 345.6376246831323,total_acc: 0.10136167705059052
[2024-09-12 02:03:57,880][train.py][line:85][INFO] ---------------epoch 79---------------
lr: [3.344195565941172e-05]
[2024-09-12 02:23:08,744][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.807803079432484
[2024-09-12 02:31:57,979][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.1395998756304437,total_acc: 0.24904991686344147
[2024-09-12 02:31:57,987][train.py][line:85][INFO] ---------------epoch 80---------------
lr: [3.307689417077162e-05]
[2024-09-12 02:51:10,641][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.840804641365364
[2024-09-12 02:59:56,803][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.6379279502831054,total_acc: 0.3294534981250763
[2024-09-12 02:59:56,811][train.py][line:85][INFO] ---------------epoch 81---------------
lr: [3.270996465959027e-05]
[2024-09-12 03:19:02,642][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.5414246758412715
[2024-09-12 03:27:49,459][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.0183836257746144,total_acc: 0.2742220461368561
[2024-09-12 03:27:49,466][train.py][line:85][INFO] ---------------epoch 82---------------
lr: [3.2341257659614606e-05]
[2024-09-12 03:46:58,058][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.578052423654064
[2024-09-12 03:55:41,494][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.4947073881557764,total_acc: 0.3529778718948364
[2024-09-12 03:55:41,501][train.py][line:85][INFO] ---------------epoch 83---------------
lr: [3.197086414314631e-05]
[2024-09-12 04:14:51,326][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.915621325987855
[2024-09-12 04:23:32,659][train.py][line:146][INFO] [testing]total_number: 142618,error: 5.809795118013804,total_acc: 0.3154650926589966
[2024-09-12 04:23:32,665][train.py][line:85][INFO] ---------------epoch 84---------------
lr: [3.1598875498595195e-05]
[2024-09-12 04:42:40,843][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.801059827074996
[2024-09-12 04:51:25,575][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.671673028005494,total_acc: 0.3448793292045593
[2024-09-12 04:51:25,582][train.py][line:85][INFO] ---------------epoch 85---------------
lr: [3.1225383507929976e-05]
[2024-09-12 05:10:41,277][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.7481723648085388
[2024-09-12 05:19:28,393][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.332756626002464,total_acc: 0.2666844427585602
[2024-09-12 05:19:28,399][train.py][line:85][INFO] ---------------epoch 86---------------
lr: [3.08504803240317e-05]
[2024-09-12 05:38:40,647][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.7128289571853093
[2024-09-12 05:47:24,890][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.678499920847809,total_acc: 0.30969443917274475
[2024-09-12 05:47:24,899][train.py][line:85][INFO] ---------------epoch 87---------------
lr: [3.0474258447955806e-05]
[2024-09-12 06:06:43,230][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.6887189696232476
[2024-09-12 06:15:28,854][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.2250651101111085,total_acc: 0.2328948676586151
[2024-09-12 06:15:28,862][train.py][line:85][INFO] ---------------epoch 88---------------
lr: [3.0096810706108264e-05]
[2024-09-12 06:34:44,620][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.668366651808489
[2024-09-12 06:43:38,523][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.082502375126526,total_acc: 0.2549748420715332
[2024-09-12 06:43:38,531][train.py][line:85][INFO] ---------------epoch 89---------------
lr: [2.9718230227341256e-05]
[2024-09-12 07:02:55,471][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.6754869053066845
[2024-09-12 07:11:51,034][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.6715619812142037,total_acc: 0.35523566603660583
[2024-09-12 07:11:51,041][train.py][line:85][INFO] ---------------epoch 90---------------
lr: [2.933861041997429e-05]
[2024-09-12 07:31:07,533][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.771902820916586
[2024-09-12 07:39:52,619][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.7550157054884887,total_acc: 0.3673799932003021
[2024-09-12 07:39:52,625][train.py][line:85][INFO] ---------------epoch 91---------------
lr: [2.895804494874642e-05]
[2024-09-12 07:58:58,668][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.5424604902985277
[2024-09-12 08:07:44,757][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.4109373318038108,total_acc: 0.354618638753891
[2024-09-12 08:07:44,763][train.py][line:85][INFO] ---------------epoch 92---------------
lr: [2.8576627711704845e-05]
[2024-09-12 08:26:56,403][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.5014015101296927
[2024-09-12 08:35:51,135][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.619682662829917,total_acc: 0.3373276889324188
[2024-09-12 08:35:51,143][train.py][line:85][INFO] ---------------epoch 93---------------
lr: [2.8194452817036296e-05]
[2024-09-12 08:55:19,658][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.720290836627765
[2024-09-12 09:04:17,560][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.8684794562317992,total_acc: 0.33697009086608887
[2024-09-12 09:04:17,569][train.py][line:85][INFO] ---------------epoch 94---------------
lr: [2.7811614559846245e-05]
[2024-09-12 09:24:03,180][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.4674486324564957
[2024-09-12 09:33:04,761][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.4915485539370112,total_acc: 0.37050020694732666
[2024-09-12 09:33:04,769][train.py][line:85][INFO] ---------------epoch 95---------------
lr: [2.7428207398892072e-05]
[2024-09-12 09:52:39,497][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.490182963857514
[2024-09-12 10:01:25,732][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.227351759817438,total_acc: 0.25934314727783203
[2024-09-12 10:01:25,739][train.py][line:85][INFO] ---------------epoch 96---------------
lr: [2.704432593327574e-05]
[2024-09-12 10:20:52,631][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.40468860316127
[2024-09-12 10:31:48,734][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.786559362859068,total_acc: 0.33413034677505493
[2024-09-12 10:31:48,742][train.py][line:85][INFO] ---------------epoch 97---------------
lr: [2.666006487910184e-05]
