[2024-09-10 13:11:59,214][train.py][line:68][INFO] ---------------args---------------
Namespace(data_path='./data/Pymatgen_Wrapped/0', train_name='ConvAtt_extend', model_path='/data/ylh/MyExps/MOFV2/checkpoints/ConvAtt_extend/ConvAtt_extend_epoch_22.pth', learning_rate=5e-05, min_learning_rate=1e-06, start_scheduler_step=0, weight_decay=1e-06, momentum=0.99, batch_size=32, class_num=230, epoch_num=200, model_save_path='/data/ylh/MyExps/MOFV2/checkpoints/ConvAtt_extend', device='1,3,5,7', scheduler_T=None, num_workers=20, log_name='log/train//train_ConvAtt_extend_2024_09_10_13:11:55.log')
[2024-09-10 13:11:59,218][train.py][line:69][INFO] ---------------model---------------
DataParallel(
  (module): ConvAtt(
    (model): Sequential(
      (0): CAtBlock(
        (conv): ResBlock1D(
          (pre): Conv1d(2, 16, kernel_size=(1,), stride=(1,), bias=False)
          (conv): Sequential(
            (0): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=16, out_features=16, bias=True)
                (WK): Linear(in_features=16, out_features=16, bias=True)
                (WV): Linear(in_features=16, out_features=16, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=16, out_features=16, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((16,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=16, out_features=32, bias=True)
                (linear2): Linear(in_features=32, out_features=16, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((16,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (1): CAtBlock(
        (conv): ResBlock1D(
          (pre): Identity()
          (conv): Sequential(
            (0): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=16, out_features=16, bias=True)
                (WK): Linear(in_features=16, out_features=16, bias=True)
                (WV): Linear(in_features=16, out_features=16, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=16, out_features=16, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((16,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=16, out_features=32, bias=True)
                (linear2): Linear(in_features=32, out_features=16, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((16,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (2): CAtBlock(
        (conv): ResBlock1D(
          (pre): Conv1d(16, 32, kernel_size=(1,), stride=(1,), bias=False)
          (conv): Sequential(
            (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=32, out_features=32, bias=True)
                (WK): Linear(in_features=32, out_features=32, bias=True)
                (WV): Linear(in_features=32, out_features=32, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=32, out_features=32, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=32, out_features=64, bias=True)
                (linear2): Linear(in_features=64, out_features=32, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (3): CAtBlock(
        (conv): ResBlock1D(
          (pre): Identity()
          (conv): Sequential(
            (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=32, out_features=32, bias=True)
                (WK): Linear(in_features=32, out_features=32, bias=True)
                (WV): Linear(in_features=32, out_features=32, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=32, out_features=32, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=32, out_features=64, bias=True)
                (linear2): Linear(in_features=64, out_features=32, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (4): CAtBlock(
        (conv): ResBlock1D(
          (pre): Conv1d(32, 64, kernel_size=(1,), stride=(1,), bias=False)
          (conv): Sequential(
            (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=64, out_features=64, bias=True)
                (WK): Linear(in_features=64, out_features=64, bias=True)
                (WV): Linear(in_features=64, out_features=64, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=64, out_features=64, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=64, out_features=128, bias=True)
                (linear2): Linear(in_features=128, out_features=64, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (5): CAtBlock(
        (conv): ResBlock1D(
          (pre): Identity()
          (conv): Sequential(
            (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=64, out_features=64, bias=True)
                (WK): Linear(in_features=64, out_features=64, bias=True)
                (WV): Linear(in_features=64, out_features=64, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=64, out_features=64, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=64, out_features=128, bias=True)
                (linear2): Linear(in_features=128, out_features=64, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (6): CAtBlock(
        (conv): ResBlock1D(
          (pre): Conv1d(64, 128, kernel_size=(1,), stride=(1,), bias=False)
          (conv): Sequential(
            (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=128, out_features=128, bias=True)
                (WK): Linear(in_features=128, out_features=128, bias=True)
                (WV): Linear(in_features=128, out_features=128, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=128, out_features=128, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=128, out_features=256, bias=True)
                (linear2): Linear(in_features=256, out_features=128, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (7): CAtBlock(
        (conv): ResBlock1D(
          (pre): Identity()
          (conv): Sequential(
            (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=128, out_features=128, bias=True)
                (WK): Linear(in_features=128, out_features=128, bias=True)
                (WV): Linear(in_features=128, out_features=128, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=128, out_features=128, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=128, out_features=256, bias=True)
                (linear2): Linear(in_features=256, out_features=128, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (8): CAtBlock(
        (conv): ResBlock1D(
          (pre): Conv1d(128, 256, kernel_size=(1,), stride=(1,), bias=False)
          (conv): Sequential(
            (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=256, out_features=256, bias=True)
                (WK): Linear(in_features=256, out_features=256, bias=True)
                (WV): Linear(in_features=256, out_features=256, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (9): CAtBlock(
        (conv): ResBlock1D(
          (pre): Identity()
          (conv): Sequential(
            (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=256, out_features=256, bias=True)
                (WK): Linear(in_features=256, out_features=256, bias=True)
                (WV): Linear(in_features=256, out_features=256, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (10): CAtBlock(
        (conv): ResBlock1D(
          (pre): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (conv): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=512, out_features=512, bias=True)
                (WK): Linear(in_features=512, out_features=512, bias=True)
                (WV): Linear(in_features=512, out_features=512, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=512, out_features=512, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (11): CAtBlock(
        (conv): ResBlock1D(
          (pre): Identity()
          (conv): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=512, out_features=512, bias=True)
                (WK): Linear(in_features=512, out_features=512, bias=True)
                (WV): Linear(in_features=512, out_features=512, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=512, out_features=512, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (12): CAtBlock(
        (conv): ResBlock1D(
          (pre): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
          (conv): Sequential(
            (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=1024, out_features=1024, bias=True)
                (WK): Linear(in_features=1024, out_features=1024, bias=True)
                (WV): Linear(in_features=1024, out_features=1024, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (13): CAtBlock(
        (conv): ResBlock1D(
          (pre): Identity()
          (conv): Sequential(
            (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=1024, out_features=1024, bias=True)
                (WK): Linear(in_features=1024, out_features=1024, bias=True)
                (WV): Linear(in_features=1024, out_features=1024, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
    )
    (cls): Linear(in_features=1024, out_features=230, bias=True)
  )
)
[2024-09-10 13:11:59,219][train.py][line:70][INFO] ---------------device---------------
cuda:1
[2024-09-10 13:11:59,219][train.py][line:71][INFO] ---------------optimizer---------------
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 5e-05
    lr: 5e-05
    maximize: False
    weight_decay: 1e-06
)
[2024-09-10 13:11:59,219][train.py][line:72][INFO] ---------------lossfn---------------
CrossEntropyLoss()
[2024-09-10 13:11:59,219][train.py][line:73][INFO] ---------------seed---------------
3407
[2024-09-10 13:11:59,223][train.py][line:85][INFO] ---------------epoch 1---------------
lr: [5e-05]
[2024-09-10 13:30:26,082][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.380699510348954
[2024-09-10 13:38:34,848][train.py][line:146][INFO] [testing]total_number: 142618,error: 1.3871619020418455,total_acc: 0.5786436200141907
[2024-09-10 13:38:35,901][train.py][line:85][INFO] ---------------epoch 2---------------
lr: [4.999395517804079e-05]
[2024-09-10 13:57:02,944][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.3202658043673603
[2024-09-10 14:05:14,274][train.py][line:146][INFO] [testing]total_number: 142618,error: 1.614421199726802,total_acc: 0.5284115672111511
[2024-09-10 14:05:14,281][train.py][line:85][INFO] ---------------epoch 3---------------
lr: [4.997884563990245e-05]
[2024-09-10 14:23:39,967][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.3319795501808966
[2024-09-10 14:31:52,783][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.0933899268187504,total_acc: 0.31918132305145264
[2024-09-10 14:31:52,789][train.py][line:85][INFO] ---------------epoch 4---------------
lr: [4.995769780401166e-05]
[2024-09-10 14:50:20,539][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.3664195384683362
[2024-09-10 14:58:32,557][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.987920850675593,total_acc: 0.10882917791604996
[2024-09-10 14:58:32,564][train.py][line:85][INFO] ---------------epoch 5---------------
lr: [4.99305168880044e-05]
[2024-09-10 15:16:54,950][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.3428383112274191
[2024-09-10 15:25:03,087][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.935805551894677,total_acc: 0.32312190532684326
[2024-09-10 15:25:03,093][train.py][line:85][INFO] ---------------epoch 6---------------
lr: [4.989730959808882e-05]
[2024-09-10 15:43:17,358][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.484147146624583
[2024-09-10 15:51:28,599][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.322130128848083,total_acc: 0.26269474625587463
[2024-09-10 15:51:28,606][train.py][line:85][INFO] ---------------epoch 7---------------
lr: [4.985808412739043e-05]
[2024-09-10 16:10:20,801][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.376663901726298
[2024-09-10 16:18:41,220][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.9827123501715267,total_acc: 0.3901330828666687
[2024-09-10 16:18:41,227][train.py][line:85][INFO] ---------------epoch 8---------------
lr: [4.9812850153930595e-05]
[2024-09-10 16:37:17,626][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.3491951011815593
[2024-09-10 16:45:34,229][train.py][line:146][INFO] [testing]total_number: 142618,error: 1.8408762959489686,total_acc: 0.535304069519043
[2024-09-10 16:45:34,236][train.py][line:85][INFO] ---------------epoch 9---------------
lr: [4.976161883823841e-05]
[2024-09-10 17:04:05,694][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.3380874469742767
[2024-09-10 17:12:34,382][train.py][line:146][INFO] [testing]total_number: 142618,error: 6.69623911888155,total_acc: 0.11491537094116211
[2024-09-10 17:12:34,389][train.py][line:85][INFO] ---------------epoch 10---------------
lr: [4.970440282059713e-05]
[2024-09-10 17:30:55,514][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.6435601802202322
[2024-09-10 17:39:19,147][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.705436668488928,total_acc: 0.3088740408420563
[2024-09-10 17:39:19,154][train.py][line:85][INFO] ---------------epoch 11---------------
lr: [4.964121621792517e-05]
[2024-09-10 17:58:15,400][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.4570857214083808
[2024-09-10 18:06:34,514][train.py][line:146][INFO] [testing]total_number: 142618,error: 4.232920550744593,total_acc: 0.21263796091079712
[2024-09-10 18:06:34,521][train.py][line:85][INFO] ---------------epoch 12---------------
lr: [4.957207462029289e-05]
[2024-09-10 18:25:21,518][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.3538173790553778
[2024-09-10 18:33:45,053][train.py][line:146][INFO] [testing]total_number: 142618,error: 1.3158835618886897,total_acc: 0.5997279286384583
[2024-09-10 18:33:45,644][train.py][line:85][INFO] ---------------epoch 13---------------
lr: [4.9496995087076e-05]
[2024-09-10 18:52:27,493][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.3752149914167688
[2024-09-10 19:00:44,220][train.py][line:146][INFO] [testing]total_number: 142618,error: 1.9182530295197255,total_acc: 0.45648515224456787
[2024-09-10 19:00:44,226][train.py][line:85][INFO] ---------------epoch 14---------------
lr: [4.941599614274625e-05]
[2024-09-10 19:19:31,351][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.3637260814000416
[2024-09-10 19:27:47,902][train.py][line:146][INFO] [testing]total_number: 142618,error: 4.994866809216878,total_acc: 0.13940036296844482
[2024-09-10 19:27:47,909][train.py][line:85][INFO] ---------------epoch 15---------------
lr: [4.932909777230082e-05]
[2024-09-10 19:46:28,830][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.373144191579633
[2024-09-10 19:54:53,687][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.316349927883422,total_acc: 0.2871516942977905
[2024-09-10 19:54:53,694][train.py][line:85][INFO] ---------------epoch 16---------------
lr: [4.923632141633112e-05]
[2024-09-10 20:13:37,840][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.0316099815043924
[2024-09-10 20:22:04,094][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.455673287926395,total_acc: 0.24368593096733093
[2024-09-10 20:22:04,101][train.py][line:85][INFO] ---------------epoch 17---------------
lr: [4.9137689965732694e-05]
[2024-09-10 20:41:45,981][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.7226611826048102
[2024-09-10 20:50:38,184][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.9780133007408045,total_acc: 0.3311363160610199
[2024-09-10 20:50:38,191][train.py][line:85][INFO] ---------------epoch 18---------------
lr: [4.903322775605711e-05]
[2024-09-10 21:10:02,560][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.6841992832533348
[2024-09-10 21:18:47,511][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.9870250071782793,total_acc: 0.34253740310668945
[2024-09-10 21:18:47,519][train.py][line:85][INFO] ---------------epoch 19---------------
lr: [4.892296056150742e-05]
[2024-09-10 21:38:06,514][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.8294446350318985
[2024-09-10 21:46:44,960][train.py][line:146][INFO] [testing]total_number: 142618,error: 6.141831718518742,total_acc: 0.10151594132184982
[2024-09-10 21:46:44,967][train.py][line:85][INFO] ---------------epoch 20---------------
lr: [4.880691558857875e-05]
[2024-09-10 22:06:16,337][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.80606351767443
[2024-09-10 22:14:44,034][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.2843412375578316,total_acc: 0.18962542712688446
[2024-09-10 22:14:44,040][train.py][line:85][INFO] ---------------epoch 21---------------
lr: [4.868512146934541e-05]
[2024-09-10 22:33:51,933][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.9623313533629567
[2024-09-10 22:42:18,389][train.py][line:146][INFO] [testing]total_number: 142618,error: 4.923997741579796,total_acc: 0.31273752450942993
[2024-09-10 22:42:18,396][train.py][line:85][INFO] ---------------epoch 22---------------
lr: [4.8557608254396214e-05]
[2024-09-10 23:03:45,339][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.902084773252835
[2024-09-10 23:12:18,586][train.py][line:146][INFO] [testing]total_number: 142618,error: 1.9897538711013119,total_acc: 0.4597315788269043
[2024-09-10 23:12:18,592][train.py][line:85][INFO] ---------------epoch 23---------------
lr: [4.842440740542e-05]
[2024-09-10 23:31:29,046][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.156352120141188
[2024-09-10 23:40:18,851][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.9650705865046882,total_acc: 0.24467457830905914
[2024-09-10 23:40:18,858][train.py][line:85][INFO] ---------------epoch 24---------------
lr: [4.8285551787442706e-05]
[2024-09-11 00:00:42,454][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.2731825262575165
[2024-09-11 00:09:11,638][train.py][line:146][INFO] [testing]total_number: 142618,error: 6.551287947376141,total_acc: 0.054845813661813736
[2024-09-11 00:09:11,645][train.py][line:85][INFO] ---------------epoch 25---------------
lr: [4.814107566071841e-05]
[2024-09-11 00:27:53,235][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.190172026901903
[2024-09-11 00:36:11,328][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.8286259444063284,total_acc: 0.19342578947544098
[2024-09-11 00:36:11,334][train.py][line:85][INFO] ---------------epoch 26---------------
lr: [4.799101467227611e-05]
[2024-09-11 00:54:57,490][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.3918381971590836
[2024-09-11 01:03:15,336][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.4306651919973365,total_acc: 0.3520102798938751
[2024-09-11 01:03:15,835][train.py][line:85][INFO] ---------------epoch 27---------------
lr: [4.7835405847124095e-05]
[2024-09-11 01:21:53,050][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.353390477544304
[2024-09-11 01:30:13,435][train.py][line:146][INFO] [testing]total_number: 142618,error: 1.9805331110527011,total_acc: 0.435835599899292
[2024-09-11 01:30:13,442][train.py][line:85][INFO] ---------------epoch 28---------------
lr: [4.767428757911467e-05]
[2024-09-11 01:48:58,369][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.286671816541623
[2024-09-11 01:57:21,046][train.py][line:146][INFO] [testing]total_number: 142618,error: 61.691062629116054,total_acc: 0.03852248564362526
[2024-09-11 01:57:21,053][train.py][line:85][INFO] ---------------epoch 29---------------
lr: [4.750769962147082e-05]
[2024-09-11 02:15:55,749][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.3583620369007083
[2024-09-11 02:24:15,769][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.338478542196708,total_acc: 0.25323590636253357
[2024-09-11 02:24:15,776][train.py][line:85][INFO] ---------------epoch 30---------------
lr: [4.733568307697772e-05]
[2024-09-11 02:43:03,093][train.py][line:103][INFO] [training]total_num: 142618.0,error: 11.334649669093256
[2024-09-11 02:51:26,058][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.795540943047479,total_acc: 0.14588621258735657
[2024-09-11 02:51:26,065][train.py][line:85][INFO] ---------------epoch 31---------------
lr: [4.715828038784109e-05]
[2024-09-11 03:10:07,032][train.py][line:103][INFO] [training]total_num: 142618.0,error: 3.2641432049774353
[2024-09-11 03:18:30,513][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.1799120023900036,total_acc: 0.22788147628307343
[2024-09-11 03:18:30,520][train.py][line:85][INFO] ---------------epoch 32---------------
lr: [4.697553532521513e-05]
[2024-09-11 03:36:58,766][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.998837811785573
[2024-09-11 03:45:10,952][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.262133928690119,total_acc: 0.1994558870792389
[2024-09-11 03:45:10,958][train.py][line:85][INFO] ---------------epoch 33---------------
lr: [4.678749297840248e-05]
[2024-09-11 04:03:40,418][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.7399353414346668
[2024-09-11 04:11:50,615][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.558813460892247,total_acc: 0.3146306872367859
[2024-09-11 04:11:50,622][train.py][line:85][INFO] ---------------epoch 34---------------
lr: [4.659419974372925e-05]
[2024-09-11 04:30:16,017][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.461330404017775
[2024-09-11 04:38:30,897][train.py][line:146][INFO] [testing]total_number: 142618,error: 4.645850051100964,total_acc: 0.1508224755525589
[2024-09-11 04:38:30,904][train.py][line:85][INFO] ---------------epoch 35---------------
lr: [4.639570331309698e-05]
[2024-09-11 04:56:55,256][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.2505693994328966
[2024-09-11 05:05:08,751][train.py][line:146][INFO] [testing]total_number: 142618,error: 7.690603818196977,total_acc: 0.0686308890581131
[2024-09-11 05:05:08,758][train.py][line:85][INFO] ---------------epoch 36---------------
lr: [4.6192052662215566e-05]
[2024-09-11 05:23:40,567][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.189095767114752
[2024-09-11 05:31:55,323][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.4154749261916324,total_acc: 0.3544573485851288
[2024-09-11 05:31:55,330][train.py][line:85][INFO] ---------------epoch 37---------------
lr: [4.598329803851896e-05]
[2024-09-11 05:50:18,296][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.45465924072757
[2024-09-11 05:58:36,065][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.360830025994436,total_acc: 0.37353628873825073
[2024-09-11 05:58:36,071][train.py][line:85][INFO] ---------------epoch 38---------------
lr: [4.576949094876734e-05]
[2024-09-11 06:17:08,795][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.2647375130097926
[2024-09-11 06:25:21,153][train.py][line:146][INFO] [testing]total_number: 142618,error: 7.1121406420050555,total_acc: 0.19937175512313843
[2024-09-11 06:25:21,160][train.py][line:85][INFO] ---------------epoch 39---------------
lr: [4.555068414633844e-05]
[2024-09-11 06:43:48,696][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.315643836039796
[2024-09-11 06:52:04,289][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.5235007694491776,total_acc: 0.3471861779689789
[2024-09-11 06:52:04,296][train.py][line:85][INFO] ---------------epoch 40---------------
lr: [4.532693161821143e-05]
[2024-09-11 07:10:30,415][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.0695804572367114
[2024-09-11 07:18:40,152][train.py][line:146][INFO] [testing]total_number: 142618,error: 10.435911889598575,total_acc: 0.3733960688114166
[2024-09-11 07:18:40,158][train.py][line:85][INFO] ---------------epoch 41---------------
lr: [4.509828857164612e-05]
[2024-09-11 07:37:05,751][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.2256987076987076
[2024-09-11 07:45:17,723][train.py][line:146][INFO] [testing]total_number: 142618,error: 80.97110184097802,total_acc: 0.04516260325908661
[2024-09-11 07:45:17,730][train.py][line:85][INFO] ---------------epoch 42---------------
lr: [4.486481142056149e-05]
[2024-09-11 08:03:50,092][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.2821762444564944
[2024-09-11 08:12:08,701][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.8202814103294442,total_acc: 0.3444516062736511
[2024-09-11 08:12:08,707][train.py][line:85][INFO] ---------------epoch 43---------------
lr: [4.4626557771616115e-05]
[2024-09-11 08:30:36,147][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.44252203204619
[2024-09-11 08:38:48,448][train.py][line:146][INFO] [testing]total_number: 142618,error: 5.366866514861156,total_acc: 0.2493864744901657
[2024-09-11 08:38:48,455][train.py][line:85][INFO] ---------------epoch 44---------------
lr: [4.438358640999466e-05]
[2024-09-11 08:57:17,700][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.8262126893072144
[2024-09-11 09:05:44,082][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.5659181589400895,total_acc: 0.33303651213645935
[2024-09-11 09:05:44,091][train.py][line:85][INFO] ---------------epoch 45---------------
lr: [4.413595728490309e-05]
[2024-09-11 09:24:29,507][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.4698728586023
[2024-09-11 09:32:57,216][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.0793132567544563,total_acc: 0.2767743170261383
[2024-09-11 09:32:57,223][train.py][line:85][INFO] ---------------epoch 46---------------
lr: [4.388373149477728e-05]
[2024-09-11 09:51:56,350][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.7852416708798393
[2024-09-11 10:00:19,913][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.1611336379388755,total_acc: 0.22432652115821838
[2024-09-11 10:00:19,920][train.py][line:85][INFO] ---------------epoch 47---------------
lr: [4.3626971272207524e-05]
[2024-09-11 10:19:04,968][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.6346919682664685
[2024-09-11 10:27:52,164][train.py][line:146][INFO] [testing]total_number: 142618,error: 37.89115902263418,total_acc: 0.29472437500953674
[2024-09-11 10:27:52,171][train.py][line:85][INFO] ---------------epoch 48---------------
lr: [4.336573996858355e-05]
