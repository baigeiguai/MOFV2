[2024-09-17 15:23:06,376][test.py][line:34][INFO] ---------------args---------------
Namespace(data_path='./data/Pymatgen_Wrapped/2/', batch_size=32, model_path='./checkpoints/AtLV2/AtLV2_epoch_16.pth', device='0', mode='test', top_k=5, parallel_model=False, test_name='TestAtLV2', num_workers=20, log_name='log/test/test_TestAtLV2_2024_09_17_15:22:57.log')
[2024-09-17 15:23:06,382][test.py][line:35][INFO] ---------------model---------------
AtLV2(
  (embed): Embedding(8501, 63, padding_idx=0)
  (att): TransformerEncoder(
    (positionEnbeding): PositionEmbedding()
    (encoder_layers): ModuleList(
      (0-7): 8 x EncoderLayer(
        (mha): MultiHeadAttention(
          (WQ): Linear(in_features=64, out_features=64, bias=True)
          (WK): Linear(in_features=64, out_features=64, bias=True)
          (WV): Linear(in_features=64, out_features=64, bias=True)
          (scaled_dot_product_attn): ScaledDotProductAttention()
          (linear): Linear(in_features=64, out_features=64, bias=True)
        )
        (dropout1): Dropout(p=0, inplace=False)
        (layernorm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
        (ffn): FeedForwardNetwork(
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (dropout2): Dropout(p=0, inplace=False)
        (layernorm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (cls): Sequential(
    (0): Linear(in_features=64, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=230, bias=True)
  )
)
[2024-09-17 15:23:06,383][test.py][line:36][INFO] ---------------device---------------
cuda:0
[2024-09-17 15:23:06,383][test.py][line:37][INFO] ---------------lossfn---------------
CrossEntropyLoss()
[2024-09-17 16:28:04,175][test.py][line:78][INFO] ---------------performance---------------
total_num:141939
error:1.876276
total_acc:0.4383714199066162
f1_score:0.13376110792160034
top5_acc:0.8280810713768005
head_acc:0.5013599541535321
medium_acc:0.21777283783458548
tail_add:0.0990835282976064

[2024-09-17 16:28:04,176][test.py][line:89][INFO] ---------------per_class_acc---------------
[0.63868093, 0.37898505, 0.0, 0.48346883, 0.44302663, 0.0, 0.106980585, 0.051136363, 0.5629458, 0.0, 0.12907375, 0.15461624, 0.2189298, 0.5463686, 0.45096782, 0.6818182, 0.0, 0.57317644, 0.8833691, 0.119205296, 0.0, 0.0, 0.16, 0.029411765, 0.0, 0.0, 0.0, 0.0, 0.23940575, 0.0, 0.025454545, 0.0, 0.42229137, 0.012903226, 0.0, 0.2536023, 0.0, 0.0, 0.0, 0.010204081, 0.0, 0.10526316, 0.32965213, 0.0, 0.028938906, 0.06944445, 0.06666667, 0.0, 0.0, 0.055555556, 0.0, 0.18983051, 0.042857144, 0.0, 0.0, 0.12147043, 0.002016129, 0.10227273, 0.007575758, 0.31102186, 0.6289772, 0.6141702, 0.11273486, 0.0, 0.009708738, 0.0, 0.0, 0.0, 0.015873017, 0.37564322, 0.04225352, 0.042056076, 0.0, 0.1958042, 0.0, 0.16101696, 0.06451613, 0.23990498, 0.0, 0.0, 0.00862069, 0.21388102, 0.0, 0.0, 0.0623608, 0.21654135, 0.11634349, 0.5990566, 0.0, 0.03846154, 0.0, 0.63179487, 0.0, 0.13207547, 0.08510638, 0.112432435, 0.0, 0.13725491, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12676056, 0.0, 0.0, 0.23021583, 0.23237179, 0.0, 0.04761905, 0.0, 0.19148937, 0.0, 0.0, 0.0, 0.0872093, 0.34615386, 0.44680852, 0.23333333, 0.035714287, 0.0, 0.12087912, 0.04494382, 0.20816326, 0.31428573, 0.0, 0.0, 0.071428575, 0.0, 0.043956045, 0.03508772, 0.08219178, 0.04347826, 0.11111111, 0.056, 0.26007327, 0.02631579, 0.38402063, 0.07772021, 0.35244754, 0.33116883, 0.79067147, 0.0, 0.0, 0.0, 0.23809524, 0.0, 0.124010555, 0.0, 0.0, 0.0, 0.13461539, 0.048648648, 0.1015625, 0.2984496, 0.10344828, 0.12345679, 0.0, 0.25974026, 0.32371795, 0.52799124, 0.0, 0.18361582, 0.08357349, 0.0, 0.0, 0.1780822, 0.0, 0.0, 0.1209068, 0.0, 0.12244898, 0.008, 0.0, 0.0, 0.15463917, 0.0, 0.0, 0.0, 0.041666668, 0.3181818, 0.0, 0.0, 0.0, 0.45454547, 0.2881356, 0.02631579, 0.12777779, 0.0, 0.44827586, 0.10714286, 0.7020548, 0.17910448, 0.0, 0.0, 0.0, 0.12727273, 0.109375, 0.5850816, 0.1971831, 0.0, 0.0, 0.0, 0.16129032, 0.0, 0.0, 0.15151516, 0.071428575, 0.0, 0.0, 0.20858896, 0.49295774, 0.0, 0.5890411, 0.11811024, 0.84615386, 0.23684211, 0.0, 0.5470588, 0.0, 0.045454547, 0.5833333, 0.0, 0.118644066]
