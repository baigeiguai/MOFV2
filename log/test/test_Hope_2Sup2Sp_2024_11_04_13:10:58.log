[2024-11-04 13:11:01,914][test.py][line:35][INFO] ---------------args---------------
Namespace(data_path='./data/Pymatgen_Wrapped/0', batch_size=128, model_path='/data/ylh/MyExps/MOFV2/checkpoints/Hope_Su2Sup/Hope_Su2Sup_epoch_104.pth', device='0', mode='test', top_k=5, parallel_model=False, test_name='Hope_2Sup2Sp', num_workers=20, balanced=False, log_name='log/test/test_Hope_2Sup2Sp_2024_11_04_13:10:58.log')
[2024-11-04 13:11:01,916][test.py][line:36][INFO] ---------------model---------------
HopeV1_Su2Sp(
  (conv_module): ResTcn(
    (intensity_norm): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (TCN): Sequential(
      (0): ResBlock1D(
        (pre): Conv1d(2, 32, kernel_size=(1,), stride=(1,), bias=False)
        (conv): Sequential(
          (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (2): ResBlock1D(
        (pre): Identity()
        (conv): Sequential(
          (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (4): ResBlock1D(
        (pre): Conv1d(32, 64, kernel_size=(1,), stride=(1,), bias=False)
        (conv): Sequential(
          (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (6): ResBlock1D(
        (pre): Conv1d(64, 128, kernel_size=(1,), stride=(1,), bias=False)
        (conv): Sequential(
          (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (7): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (8): ResBlock1D(
        (pre): Identity()
        (conv): Sequential(
          (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (9): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
      (10): ResBlock1D(
        (pre): Conv1d(128, 256, kernel_size=(1,), stride=(1,), bias=False)
        (conv): Sequential(
          (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (11): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
      (12): ResBlock1D(
        (pre): Identity()
        (conv): Sequential(
          (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (13): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
      (14): ResBlock1D(
        (pre): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
        (conv): Sequential(
          (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (15): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
      (16): ResBlock1D(
        (pre): Identity()
        (conv): Sequential(
          (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (17): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
      (18): ResBlock1D(
        (pre): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
        (conv): Sequential(
          (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (19): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
      (20): Dropout(p=0.25, inplace=False)
      (21): ResBlock1D(
        (pre): Identity()
        (conv): Sequential(
          (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (22): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
      (23): Dropout(p=0.25, inplace=False)
      (24): ResBlock1D(
        (pre): Identity()
        (conv): Sequential(
          (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (25): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
      (26): Dropout(p=0.25, inplace=False)
      (27): ResBlock1D(
        (pre): Identity()
        (conv): Sequential(
          (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (28): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
      (29): Flatten(start_dim=1, end_dim=-1)
    )
  )
  (att): AttentionModule(
    (embed): Embedding(850, 15)
    (patch_conv): PatchConvModule(
      (conv): Sequential(
        (0): Conv2d(1, 2, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        (1): AvgPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0)
        (2): Conv2d(2, 4, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        (3): AvgPool2d(kernel_size=(1, 2), stride=(1, 2), padding=(0, 1))
        (4): Conv2d(4, 8, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        (5): AvgPool2d(kernel_size=(1, 2), stride=(1, 2), padding=(0, 1))
        (6): Conv2d(8, 16, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        (7): AvgPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0)
      )
    )
    (att): TransformerEncoder(
      (positionEnbeding): PositionEmbedding()
      (encoder_layers): ModuleList(
        (0-7): 8 x EncoderLayer(
          (mha): MultiHeadAttention(
            (WQ): Linear(in_features=32, out_features=32, bias=True)
            (WK): Linear(in_features=32, out_features=32, bias=True)
            (WV): Linear(in_features=32, out_features=32, bias=True)
            (scaled_dot_product_attn): ScaledDotProductAttention()
            (linear): Linear(in_features=32, out_features=32, bias=True)
          )
          (dropout1): Dropout(p=0.25, inplace=False)
          (layernorm1): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (linear1): Linear(in_features=32, out_features=256, bias=True)
            (linear2): Linear(in_features=256, out_features=32, bias=True)
            (relu): LeakyReLU(negative_slope=0.01)
          )
          (dropout2): Dropout(p=0.25, inplace=False)
          (layernorm2): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
  )
  (cls_sp): Sequential(
    (0): Linear(in_features=1069, out_features=230, bias=True)
  )
  (cls_cs): Sequential(
    (0): Linear(in_features=1056, out_features=7, bias=True)
  )
  (cls_lt): Sequential(
    (0): Linear(in_features=1056, out_features=6, bias=True)
  )
)
[2024-11-04 13:11:01,916][test.py][line:37][INFO] ---------------device---------------
cuda:0
[2024-11-04 13:11:01,916][test.py][line:38][INFO] ---------------lossfn---------------
CrossEntropyLoss()
[2024-11-04 13:13:53,534][test.py][line:89][INFO] ---------------performance---------------
total_num:142729
error:1.174996
total_acc:0.7950101494789124
f1_score:0.5642708539962769
top5_acc:0.9650106430053711
head_acc:0.8090196779863956
medium_acc:0.601117070128755
tail_add:0.4264182050131533
crystal_system_acc:0.9302804470062256
lattice_type_acc:0.9392134547233582

[2024-11-04 13:13:53,535][test.py][line:102][INFO] ---------------per_class_acc---------------
[0.7851586, 0.7764469, 0.17105263, 0.87745625, 0.8323327, 0.33333334, 0.66625565, 0.4090909, 0.811978, 0.3114754, 0.77919865, 0.63384956, 0.6280125, 0.8298324, 0.7835773, 0.6956522, 0.4473684, 0.79841715, 0.94349134, 0.7844639, 0.25, 0.05263158, 0.6614173, 0.2647059, 0.0, 0.47368422, 0.23529412, 0.33333334, 0.796742, 0.36619717, 0.6557971, 0.32222223, 0.82745576, 0.38709676, 0.4, 0.7132565, 0.3898305, 0.5, 0.0, 0.35714287, 0.0, 0.55263156, 0.8654164, 0.43243244, 0.63897765, 0.5555556, 0.6666667, 0.35135135, 0.5, 0.37037036, 0.30769232, 0.6108291, 0.6056338, 0.42578125, 0.3576159, 0.65802014, 0.62575454, 0.62711865, 0.48120302, 0.7601689, 0.95200086, 0.845, 0.64033264, 0.0, 0.66019416, 0.3181818, 0.0, 0.0, 0.46031746, 0.792517, 0.5416667, 0.6976744, 0.53503186, 0.7622378, 0.27586207, 0.5487288, 0.33870968, 0.33254156, 0.17910448, 0.68503934, 0.67241377, 0.8328612, 0.1875, 0.5416667, 0.7951002, 0.8828829, 0.72853184, 0.9357702, 0.0, 0.3846154, 0.15555556, 0.32822084, 0.0, 0.5283019, 0.42553192, 0.6842672, 0.29166666, 0.8039216, 0.0, 0.0, 0.0, 0.3846154, 0.47058824, 0.72131145, 1.0, 0.6603774, 0.083333336, 0.31578946, 0.17391305, 0.8309859, 0.0, 0.23529412, 0.76258993, 0.9264, 0.0, 0.2857143, 0.6060606, 0.67021275, 0.3125, 0.4642857, 0.47126436, 0.79130435, 0.78205127, 0.87234044, 0.93333334, 0.76785713, 0.47619048, 0.73626375, 0.7977528, 0.91463417, 0.78571427, 0.3, 0.32, 0.5714286, 0.42, 0.7173913, 0.6315789, 0.739726, 0.45217392, 0.6388889, 0.7222222, 0.83882785, 0.5, 0.46649486, 0.4326425, 0.76256984, 0.76661265, 0.9390052, 0.33333334, 0.18518518, 0.0, 0.46169773, 0.09090909, 0.40575916, 0.3493976, 0.0, 0.11111111, 0.3653846, 0.6864865, 0.5546875, 0.874031, 0.55172414, 0.7407407, 0.4489796, 0.87792206, 0.7284345, 0.91885966, 0.23076923, 0.32394367, 0.57060516, 0.125, 0.032258064, 0.73224044, 0.0625, 0.375, 0.7613065, 0.6666667, 0.60544217, 0.304, 0.325, 0.44, 0.5154639, 0.0, 0.2857143, 0.2631579, 0.33333334, 0.6363636, 0.375, 0.3181818, 0.39772728, 0.6515151, 0.8305085, 0.65789473, 0.76243097, 0.2, 0.8448276, 0.64285713, 0.9589041, 0.58208954, 0.23076923, 0.56, 0.16666667, 0.76363635, 0.625, 0.997669, 0.943662, 0.61538464, 0.33333334, 0.37142858, 0.7419355, 0.6896552, 0.7083333, 0.57575756, 0.85714287, 0.45454547, 0.09090909, 0.7791411, 0.943662, 0.8125, 1.0, 0.88188976, 1.0, 0.84210527, 0.8125, 0.9352941, 0.8333333, 0.8636364, 0.9861111, 0.3181818, 1.0]
