[2024-09-18 16:51:02,249][test.py][line:34][INFO] ---------------args---------------
Namespace(data_path='./data/Pymatgen_Wrapped/2', batch_size=128, model_path='/data/ylh/MyExps/MOFV2/checkpoints/AtLSmall_2layer/AtLSmall_2layer_epoch_40.pth', device='2', mode='test', top_k=5, parallel_model=False, test_name='AtLSmall_2layer', num_workers=20, log_name='log/test/test_AtLSmall_2layer_2024_09_18_16:50:57.log')
[2024-09-18 16:51:02,250][test.py][line:35][INFO] ---------------model---------------
AtLSmall(
  (embed): Embedding(8501, 31, padding_idx=0)
  (att): TransformerEncoder(
    (positionEnbeding): PositionEmbedding()
    (encoder_layers): ModuleList(
      (0-1): 2 x EncoderLayer(
        (mha): MultiHeadAttention(
          (WQ): Linear(in_features=320, out_features=320, bias=True)
          (WK): Linear(in_features=320, out_features=320, bias=True)
          (WV): Linear(in_features=320, out_features=320, bias=True)
          (scaled_dot_product_attn): ScaledDotProductAttention()
          (linear): Linear(in_features=320, out_features=320, bias=True)
        )
        (dropout1): Dropout(p=0, inplace=False)
        (layernorm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (ffn): FeedForwardNetwork(
          (linear1): Linear(in_features=320, out_features=512, bias=True)
          (linear2): Linear(in_features=512, out_features=320, bias=True)
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (dropout2): Dropout(p=0, inplace=False)
        (layernorm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (cls): Sequential(
    (0): Linear(in_features=320, out_features=256, bias=True)
    (1): Linear(in_features=256, out_features=230, bias=True)
  )
)
[2024-09-18 16:51:02,250][test.py][line:36][INFO] ---------------device---------------
cuda:2
[2024-09-18 16:51:02,250][test.py][line:37][INFO] ---------------lossfn---------------
CrossEntropyLoss()
[2024-09-18 16:52:48,815][test.py][line:78][INFO] ---------------performance---------------
total_num:141939
error:6.374386
total_acc:0.24011018872261047
f1_score:0.11091651767492294
top5_acc:0.6005678772926331
head_acc:0.2694042728651838
medium_acc:0.1379689721135169
tail_add:0.08502097227617846

[2024-09-18 16:52:48,816][test.py][line:89][INFO] ---------------per_class_acc---------------
[0.36212826, 0.3275862, 0.0, 0.25636858, 0.21356615, 0.055555556, 0.123089634, 0.14772727, 0.29012126, 0.016666668, 0.12692967, 0.11568409, 0.093210585, 0.3897743, 0.26489145, 0.6818182, 0.0, 0.19905883, 0.40751073, 0.11037528, 0.0, 0.055555556, 0.016, 0.029411765, 0.0, 0.05263158, 0.05882353, 0.0, 0.1285923, 0.04225352, 0.087272726, 0.033333335, 0.19599621, 0.07096774, 0.2, 0.12536024, 0.01724138, 0.045454547, 0.0, 0.030612245, 0.0, 0.078947365, 0.1606847, 0.027027028, 0.06752411, 0.055555556, 0.06666667, 0.0, 0.0, 0.018518519, 0.0, 0.055932205, 0.0, 0.007874016, 0.02, 0.06925946, 0.044354837, 0.056818184, 0.022727273, 0.117135204, 0.29147983, 0.20369658, 0.0960334, 0.0, 0.097087376, 0.0, 0.0, 0.0, 0.0952381, 0.09090909, 0.07042254, 0.05140187, 0.0, 0.034965035, 0.0, 0.15889831, 0.0, 0.06175772, 0.04477612, 0.05511811, 0.060344826, 0.2549575, 0.0, 0.0, 0.10467706, 0.06616541, 0.1301939, 0.2321803, 0.0, 0.09615385, 0.0, 0.11897436, 0.0, 0.047169812, 0.0, 0.12756757, 0.0, 0.27450982, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.083333336, 0.0, 0.04347826, 0.061032865, 0.0, 0.0, 0.18705036, 0.11378205, 0.0, 0.0952381, 0.121212125, 0.06382979, 0.0625, 0.035714287, 0.09195402, 0.034883723, 0.12820514, 0.23404256, 0.1, 0.05357143, 0.04761905, 0.021978023, 0.04494382, 0.057142857, 0.08571429, 0.0, 0.0, 0.035714287, 0.0, 0.021978023, 0.0, 0.06849315, 0.07826087, 0.027777778, 0.072, 0.05860806, 0.07017544, 0.0927835, 0.19170985, 0.25174826, 0.2159091, 0.4747634, 0.16666667, 0.0, 0.0, 0.099378884, 0.0, 0.07651715, 0.072289154, 0.0, 0.11111111, 0.057692308, 0.1027027, 0.25, 0.069767445, 0.06896552, 0.17695473, 0.1632653, 0.25194806, 0.27564102, 0.19978046, 0.0, 0.12711865, 0.04610951, 0.05, 0.032258064, 0.043835618, 0.0, 0.041666668, 0.14357683, 0.0, 0.027210884, 0.032, 0.0, 0.04, 0.25773194, 0.0, 0.0, 0.0, 0.041666668, 0.27272728, 0.0, 0.0, 0.011363637, 0.13636364, 0.40677965, 0.02631579, 0.083333336, 0.1, 0.27586207, 0.0952381, 0.53424656, 0.29850745, 0.07692308, 0.0, 0.041666668, 0.10909091, 0.21875, 0.47319347, 0.09859155, 0.07692308, 0.0, 0.057142857, 0.12903225, 0.20689656, 0.125, 0.21212122, 0.0, 0.045454547, 0.0, 0.49079755, 0.26760563, 0.125, 0.2739726, 0.37007874, 0.37179488, 0.078947365, 0.125, 0.5735294, 0.11904762, 0.14772727, 0.31944445, 0.27272728, 0.20338982]
