[2024-09-09 20:06:47,311][train.py][line:68][INFO] ---------------args---------------
Namespace(data_path='./data/Pymatgen_Wrapped/0', train_name='ConvAtt_extend', model_path='/data/ylh/MyExps/MOFV2/checkpoints/ConvAtt/ConvAtt_epoch_46.pth', learning_rate=0.0001, min_learning_rate=1e-05, start_scheduler_step=0, weight_decay=1e-06, momentum=0.99, batch_size=32, class_num=230, epoch_num=200, model_save_path='/data/ylh/MyExps/MOFV2/checkpoints/ConvAtt_extend', device='1,3,5,7', scheduler_T=None, num_workers=20, log_name='log/train//train_ConvAtt_extend_2024_09_09_20:06:42.log')
[2024-09-09 20:06:47,317][train.py][line:69][INFO] ---------------model---------------
DataParallel(
  (module): ConvAtt(
    (model): Sequential(
      (0): CAtBlock(
        (conv): ResBlock1D(
          (pre): Conv1d(2, 16, kernel_size=(1,), stride=(1,), bias=False)
          (conv): Sequential(
            (0): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=16, out_features=16, bias=True)
                (WK): Linear(in_features=16, out_features=16, bias=True)
                (WV): Linear(in_features=16, out_features=16, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=16, out_features=16, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((16,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=16, out_features=32, bias=True)
                (linear2): Linear(in_features=32, out_features=16, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((16,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (1): CAtBlock(
        (conv): ResBlock1D(
          (pre): Identity()
          (conv): Sequential(
            (0): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=16, out_features=16, bias=True)
                (WK): Linear(in_features=16, out_features=16, bias=True)
                (WV): Linear(in_features=16, out_features=16, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=16, out_features=16, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((16,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=16, out_features=32, bias=True)
                (linear2): Linear(in_features=32, out_features=16, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((16,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (2): CAtBlock(
        (conv): ResBlock1D(
          (pre): Conv1d(16, 32, kernel_size=(1,), stride=(1,), bias=False)
          (conv): Sequential(
            (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=32, out_features=32, bias=True)
                (WK): Linear(in_features=32, out_features=32, bias=True)
                (WV): Linear(in_features=32, out_features=32, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=32, out_features=32, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=32, out_features=64, bias=True)
                (linear2): Linear(in_features=64, out_features=32, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (3): CAtBlock(
        (conv): ResBlock1D(
          (pre): Identity()
          (conv): Sequential(
            (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=32, out_features=32, bias=True)
                (WK): Linear(in_features=32, out_features=32, bias=True)
                (WV): Linear(in_features=32, out_features=32, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=32, out_features=32, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=32, out_features=64, bias=True)
                (linear2): Linear(in_features=64, out_features=32, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (4): CAtBlock(
        (conv): ResBlock1D(
          (pre): Conv1d(32, 64, kernel_size=(1,), stride=(1,), bias=False)
          (conv): Sequential(
            (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=64, out_features=64, bias=True)
                (WK): Linear(in_features=64, out_features=64, bias=True)
                (WV): Linear(in_features=64, out_features=64, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=64, out_features=64, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=64, out_features=128, bias=True)
                (linear2): Linear(in_features=128, out_features=64, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (5): CAtBlock(
        (conv): ResBlock1D(
          (pre): Identity()
          (conv): Sequential(
            (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=64, out_features=64, bias=True)
                (WK): Linear(in_features=64, out_features=64, bias=True)
                (WV): Linear(in_features=64, out_features=64, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=64, out_features=64, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=64, out_features=128, bias=True)
                (linear2): Linear(in_features=128, out_features=64, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (6): CAtBlock(
        (conv): ResBlock1D(
          (pre): Conv1d(64, 128, kernel_size=(1,), stride=(1,), bias=False)
          (conv): Sequential(
            (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=128, out_features=128, bias=True)
                (WK): Linear(in_features=128, out_features=128, bias=True)
                (WV): Linear(in_features=128, out_features=128, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=128, out_features=128, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=128, out_features=256, bias=True)
                (linear2): Linear(in_features=256, out_features=128, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (7): CAtBlock(
        (conv): ResBlock1D(
          (pre): Identity()
          (conv): Sequential(
            (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=128, out_features=128, bias=True)
                (WK): Linear(in_features=128, out_features=128, bias=True)
                (WV): Linear(in_features=128, out_features=128, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=128, out_features=128, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=128, out_features=256, bias=True)
                (linear2): Linear(in_features=256, out_features=128, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (8): CAtBlock(
        (conv): ResBlock1D(
          (pre): Conv1d(128, 256, kernel_size=(1,), stride=(1,), bias=False)
          (conv): Sequential(
            (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=256, out_features=256, bias=True)
                (WK): Linear(in_features=256, out_features=256, bias=True)
                (WV): Linear(in_features=256, out_features=256, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (9): CAtBlock(
        (conv): ResBlock1D(
          (pre): Identity()
          (conv): Sequential(
            (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=256, out_features=256, bias=True)
                (WK): Linear(in_features=256, out_features=256, bias=True)
                (WV): Linear(in_features=256, out_features=256, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (10): CAtBlock(
        (conv): ResBlock1D(
          (pre): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (conv): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=512, out_features=512, bias=True)
                (WK): Linear(in_features=512, out_features=512, bias=True)
                (WV): Linear(in_features=512, out_features=512, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=512, out_features=512, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (11): CAtBlock(
        (conv): ResBlock1D(
          (pre): Identity()
          (conv): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=512, out_features=512, bias=True)
                (WK): Linear(in_features=512, out_features=512, bias=True)
                (WV): Linear(in_features=512, out_features=512, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=512, out_features=512, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (12): CAtBlock(
        (conv): ResBlock1D(
          (pre): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
          (conv): Sequential(
            (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=1024, out_features=1024, bias=True)
                (WK): Linear(in_features=1024, out_features=1024, bias=True)
                (WV): Linear(in_features=1024, out_features=1024, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
      (13): CAtBlock(
        (conv): ResBlock1D(
          (pre): Identity()
          (conv): Sequential(
            (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): LeakyReLU(negative_slope=0.01)
            (3): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
        (att): TransformerEncoder(
          (positionEnbeding): PositionEmbedding()
          (encoder_layers): ModuleList(
            (0): EncoderLayer(
              (mha): MultiHeadAttention(
                (WQ): Linear(in_features=1024, out_features=1024, bias=True)
                (WK): Linear(in_features=1024, out_features=1024, bias=True)
                (WV): Linear(in_features=1024, out_features=1024, bias=True)
                (scaled_dot_product_attn): ScaledDotProductAttention()
                (linear): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (dropout1): Dropout(p=0, inplace=False)
              (layernorm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
              (ffn): FeedForwardNetwork(
                (linear1): Linear(in_features=1024, out_features=2048, bias=True)
                (linear2): Linear(in_features=2048, out_features=1024, bias=True)
                (relu): LeakyReLU(negative_slope=0.01)
              )
              (dropout2): Dropout(p=0, inplace=False)
              (layernorm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
    )
    (cls): Linear(in_features=1024, out_features=230, bias=True)
  )
)
[2024-09-09 20:06:47,318][train.py][line:70][INFO] ---------------device---------------
cuda:1
[2024-09-09 20:06:47,318][train.py][line:71][INFO] ---------------optimizer---------------
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0001
    lr: 0.0001
    maximize: False
    weight_decay: 1e-06
)
[2024-09-09 20:06:47,318][train.py][line:72][INFO] ---------------lossfn---------------
CrossEntropyLoss()
[2024-09-09 20:06:47,318][train.py][line:73][INFO] ---------------seed---------------
3407
[2024-09-09 20:06:47,332][train.py][line:85][INFO] ---------------epoch 1---------------
lr: [0.0001]
[2024-09-09 20:25:15,603][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.5730828813086914
[2024-09-09 20:33:20,397][train.py][line:146][INFO] [testing]total_number: 142618,error: 1.8087225389336385,total_acc: 0.4460727274417877
[2024-09-09 20:33:21,412][train.py][line:85][INFO] ---------------epoch 2---------------
lr: [9.998889726578921e-05]
[2024-09-09 20:52:56,278][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.5991531348516863
[2024-09-09 21:01:35,906][train.py][line:146][INFO] [testing]total_number: 142618,error: 1.807804039782948,total_acc: 0.4466336667537689
[2024-09-09 21:01:36,459][train.py][line:85][INFO] ---------------epoch 3---------------
lr: [9.996114505288205e-05]
[2024-09-09 21:21:01,584][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.5036978669811747
[2024-09-09 21:29:07,021][train.py][line:146][INFO] [testing]total_number: 142618,error: 1.7574356988366147,total_acc: 0.5088838934898376
[2024-09-09 21:29:07,538][train.py][line:85][INFO] ---------------epoch 4---------------
lr: [9.9922302089001e-05]
[2024-09-09 21:47:22,836][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.5209269636581022
[2024-09-09 21:55:26,761][train.py][line:146][INFO] [testing]total_number: 142618,error: 5.814439687196926,total_acc: 0.0887686014175415
[2024-09-09 21:55:26,767][train.py][line:85][INFO] ---------------epoch 5---------------
lr: [9.98723779575591e-05]
[2024-09-09 22:14:47,908][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.4597867621262441
[2024-09-09 22:23:24,809][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.2286691189506573,total_acc: 0.36714860796928406
[2024-09-09 22:23:24,816][train.py][line:85][INFO] ---------------epoch 6---------------
lr: [9.98113849760815e-05]
[2024-09-09 22:43:04,217][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.440162672484327
[2024-09-09 22:51:45,399][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.3281559491242985,total_acc: 0.3922436237335205
[2024-09-09 22:51:45,406][train.py][line:85][INFO] ---------------epoch 7---------------
lr: [9.973933819316609e-05]
[2024-09-09 23:12:05,440][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.4400657498841858
[2024-09-09 23:21:07,419][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.4391526360078095,total_acc: 0.2552412748336792
[2024-09-09 23:21:07,427][train.py][line:85][INFO] ---------------epoch 8---------------
lr: [9.965625538477046e-05]
[2024-09-09 23:41:24,060][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.4151703213144589
[2024-09-09 23:50:53,104][train.py][line:146][INFO] [testing]total_number: 142618,error: 5.354384009983377,total_acc: 0.131385937333107
[2024-09-09 23:50:53,111][train.py][line:85][INFO] ---------------epoch 9---------------
lr: [9.956215704982564e-05]
[2024-09-10 00:10:45,354][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.440955633317591
[2024-09-10 00:19:01,042][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.3541253021380806,total_acc: 0.25641223788261414
[2024-09-10 00:19:01,050][train.py][line:85][INFO] ---------------epoch 10---------------
lr: [9.945706640517837e-05]
[2024-09-10 00:37:47,983][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.4366630574014025
[2024-09-10 00:45:57,422][train.py][line:146][INFO] [testing]total_number: 142618,error: 7.524380982776696,total_acc: 0.07557250559329987
[2024-09-10 00:45:57,429][train.py][line:85][INFO] ---------------epoch 11---------------
lr: [9.934100937986254e-05]
[2024-09-10 01:04:19,134][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.466854047190438
[2024-09-10 01:12:22,920][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.073246654617103,total_acc: 0.25533244013786316
[2024-09-10 01:12:22,927][train.py][line:85][INFO] ---------------epoch 12---------------
lr: [9.92140146087012e-05]
[2024-09-10 01:30:45,090][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.4325249998105898
[2024-09-10 01:38:48,737][train.py][line:146][INFO] [testing]total_number: 142618,error: 1.4507425227018882,total_acc: 0.5626428723335266
[2024-09-10 01:38:49,260][train.py][line:85][INFO] ---------------epoch 13---------------
lr: [9.907611342524162e-05]
[2024-09-10 01:57:16,142][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.468575695936825
[2024-09-10 02:05:21,590][train.py][line:146][INFO] [testing]total_number: 142618,error: 5.231103009540975,total_acc: 0.05582745373249054
[2024-09-10 02:05:21,597][train.py][line:85][INFO] ---------------epoch 14---------------
lr: [9.892733985402372e-05]
[2024-09-10 02:23:47,180][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.4607919292477725
[2024-09-10 02:31:50,740][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.4507237803658275,total_acc: 0.30005329847335815
[2024-09-10 02:31:50,746][train.py][line:85][INFO] ---------------epoch 15---------------
lr: [9.876773060218517e-05]
[2024-09-10 02:50:18,814][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.4246466990733873
[2024-09-10 02:58:24,233][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.4616347088288237,total_acc: 0.19688959419727325
[2024-09-10 02:58:24,240][train.py][line:85][INFO] ---------------epoch 16---------------
lr: [9.859732505040408e-05]
[2024-09-10 03:16:54,177][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.3990502667656723
[2024-09-10 03:25:00,683][train.py][line:146][INFO] [testing]total_number: 142618,error: 5.113479761773967,total_acc: 0.17553183436393738
[2024-09-10 03:25:00,689][train.py][line:85][INFO] ---------------epoch 17---------------
lr: [9.841616524318249e-05]
[2024-09-10 03:43:25,992][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.4515261374227035
[2024-09-10 03:51:30,503][train.py][line:146][INFO] [testing]total_number: 142618,error: 6.665639191812512,total_acc: 0.11593908071517944
[2024-09-10 03:51:30,509][train.py][line:85][INFO] ---------------epoch 18---------------
lr: [9.822429587847222e-05]
[2024-09-10 04:09:59,035][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.4817814587692206
[2024-09-10 04:18:02,566][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.1504238232382735,total_acc: 0.281268835067749
[2024-09-10 04:18:02,571][train.py][line:85][INFO] ---------------epoch 19---------------
lr: [9.802176429664626e-05]
[2024-09-10 04:36:25,768][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.4523055970027883
[2024-09-10 04:44:30,002][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.1194416802630203,total_acc: 0.3719305992126465
[2024-09-10 04:44:30,009][train.py][line:85][INFO] ---------------epoch 20---------------
lr: [9.78086204688181e-05]
[2024-09-10 05:02:57,032][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.4762570354753712
[2024-09-10 05:11:01,273][train.py][line:146][INFO] [testing]total_number: 142618,error: 6.822274838777854,total_acc: 0.07418418675661087
[2024-09-10 05:11:01,281][train.py][line:85][INFO] ---------------epoch 21---------------
lr: [9.758491698451196e-05]
[2024-09-10 05:29:30,837][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.4963226057589054
[2024-09-10 05:37:34,435][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.363684019173986,total_acc: 0.32016295194625854
[2024-09-10 05:37:34,441][train.py][line:85][INFO] ---------------epoch 22---------------
lr: [9.735070903868689e-05]
[2024-09-10 05:55:57,938][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.48629137538507
[2024-09-10 06:04:01,914][train.py][line:146][INFO] [testing]total_number: 142618,error: 1.4317279221843862,total_acc: 0.5693811178207397
[2024-09-10 06:04:02,438][train.py][line:85][INFO] ---------------epoch 23---------------
lr: [9.710605441811833e-05]
[2024-09-10 06:22:37,432][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.4982896233678504
[2024-09-10 06:30:41,713][train.py][line:146][INFO] [testing]total_number: 142618,error: 1.6103381242563006,total_acc: 0.5062193870544434
[2024-09-10 06:30:41,721][train.py][line:85][INFO] ---------------epoch 24---------------
lr: [9.685101348713963e-05]
[2024-09-10 06:49:07,030][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.5385058016973585
[2024-09-10 06:57:13,767][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.735406797686358,total_acc: 0.22218093276023865
[2024-09-10 06:57:13,773][train.py][line:85][INFO] ---------------epoch 25---------------
lr: [9.658564917274808e-05]
[2024-09-10 07:15:45,898][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.6166128968305913
[2024-09-10 07:23:54,884][train.py][line:146][INFO] [testing]total_number: 142618,error: 1.6359693905118333,total_acc: 0.5221220254898071
[2024-09-10 07:23:54,890][train.py][line:85][INFO] ---------------epoch 26---------------
lr: [9.631002694907854e-05]
[2024-09-10 07:42:27,649][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.7059020865508305
[2024-09-10 07:50:29,499][train.py][line:146][INFO] [testing]total_number: 142618,error: 1.680039282607776,total_acc: 0.5693110227584839
[2024-09-10 07:50:29,985][train.py][line:85][INFO] ---------------epoch 27---------------
lr: [9.60242148212483e-05]
[2024-09-10 08:09:00,763][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.4775729979985932
[2024-09-10 08:16:57,240][train.py][line:146][INFO] [testing]total_number: 142618,error: 1.769445446723785,total_acc: 0.5150401592254639
[2024-09-10 08:16:57,247][train.py][line:85][INFO] ---------------epoch 28---------------
lr: [9.572828330857793e-05]
[2024-09-10 08:35:53,291][train.py][line:103][INFO] [training]total_num: 142618.0,error: 1.7495585378112546
[2024-09-10 08:44:03,667][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.202201239924918,total_acc: 0.3344598710536957
[2024-09-10 08:44:03,674][train.py][line:85][INFO] ---------------epoch 29---------------
lr: [9.542230542719128e-05]
[2024-09-10 09:02:55,525][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.122624343948766
[2024-09-10 09:11:54,932][train.py][line:146][INFO] [testing]total_number: 142618,error: 4.780053318134345,total_acc: 0.18293623626232147
[2024-09-10 09:11:54,938][train.py][line:85][INFO] ---------------epoch 30---------------
lr: [9.510635667199987e-05]
[2024-09-10 09:31:49,020][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.279959015534877
[2024-09-10 09:40:36,479][train.py][line:146][INFO] [testing]total_number: 142618,error: 4.139239109491789,total_acc: 0.3589308559894562
[2024-09-10 09:40:36,485][train.py][line:85][INFO] ---------------epoch 31---------------
lr: [9.478051499807545e-05]
[2024-09-10 10:00:59,594][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.112636634846315
[2024-09-10 10:09:13,067][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.18761786956033,total_acc: 0.42824888229370117
[2024-09-10 10:09:13,075][train.py][line:85][INFO] ---------------epoch 32---------------
lr: [9.44448608014155e-05]
[2024-09-10 10:27:20,296][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.3658913829794495
[2024-09-10 10:35:16,054][train.py][line:146][INFO] [testing]total_number: 142618,error: 2.36913744002367,total_acc: 0.39455747604370117
[2024-09-10 10:35:16,062][train.py][line:85][INFO] ---------------epoch 33---------------
lr: [9.409947689910657e-05]
[2024-09-10 10:53:28,015][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.457659669834653
[2024-09-10 11:01:21,552][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.3730937290042107,total_acc: 0.22233518958091736
[2024-09-10 11:01:21,558][train.py][line:85][INFO] ---------------epoch 34---------------
lr: [9.374444850889043e-05]
[2024-09-10 11:19:28,635][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.7909508064709683
[2024-09-10 11:27:21,450][train.py][line:146][INFO] [testing]total_number: 142618,error: 19.92959350045757,total_acc: 0.28789493441581726
[2024-09-10 11:27:21,456][train.py][line:85][INFO] ---------------epoch 35---------------
lr: [9.337986322813729e-05]
[2024-09-10 11:45:26,391][train.py][line:103][INFO] [training]total_num: 142618.0,error: 3.363356286052307
[2024-09-10 11:53:24,129][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.5216201745160594,total_acc: 0.21153010427951813
[2024-09-10 11:53:24,136][train.py][line:85][INFO] ---------------epoch 36---------------
lr: [9.300581101223264e-05]
[2024-09-10 12:11:24,159][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.8300114150115667
[2024-09-10 12:19:18,813][train.py][line:146][INFO] [testing]total_number: 142618,error: 5.162064662116403,total_acc: 0.23871460556983948
[2024-09-10 12:19:18,820][train.py][line:85][INFO] ---------------epoch 37---------------
lr: [9.262238415238172e-05]
[2024-09-10 12:37:30,818][train.py][line:103][INFO] [training]total_num: 142618.0,error: 2.873851329034802
[2024-09-10 12:45:25,757][train.py][line:146][INFO] [testing]total_number: 142618,error: 3.4205369308163616,total_acc: 0.2796982228755951
[2024-09-10 12:45:25,763][train.py][line:85][INFO] ---------------epoch 38---------------
lr: [9.222967725283793e-05]
[2024-09-10 13:03:33,428][train.py][line:103][INFO] [training]total_num: 142618.0,error: 3.006433561230646
