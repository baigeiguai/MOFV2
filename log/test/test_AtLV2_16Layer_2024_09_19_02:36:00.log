[2024-09-19 02:36:08,619][test.py][line:34][INFO] ---------------args---------------
Namespace(data_path='./data/Pymatgen_Wrapped/2/', batch_size=256, model_path='./checkpoints/AtLV2_16L/AtLV2_16layer_epoch_32.pth', device='0', mode='test', top_k=5, parallel_model=False, test_name='AtLV2_16Layer', num_workers=20, log_name='log/test/test_AtLV2_16Layer_2024_09_19_02:36:00.log')
[2024-09-19 02:36:08,624][test.py][line:35][INFO] ---------------model---------------
AtLSmall(
  (embed): Embedding(8501, 31, padding_idx=0)
  (att): TransformerEncoder(
    (positionEnbeding): PositionEmbedding()
    (encoder_layers): ModuleList(
      (0-1): 2 x EncoderLayer(
        (mha): MultiHeadAttention(
          (WQ): Linear(in_features=320, out_features=320, bias=True)
          (WK): Linear(in_features=320, out_features=320, bias=True)
          (WV): Linear(in_features=320, out_features=320, bias=True)
          (scaled_dot_product_attn): ScaledDotProductAttention()
          (linear): Linear(in_features=320, out_features=320, bias=True)
        )
        (dropout1): Dropout(p=0, inplace=False)
        (layernorm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (ffn): FeedForwardNetwork(
          (linear1): Linear(in_features=320, out_features=512, bias=True)
          (linear2): Linear(in_features=512, out_features=320, bias=True)
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (dropout2): Dropout(p=0, inplace=False)
        (layernorm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (cls): Sequential(
    (0): Linear(in_features=320, out_features=256, bias=True)
    (1): Linear(in_features=256, out_features=230, bias=True)
  )
)
[2024-09-19 02:36:08,624][test.py][line:36][INFO] ---------------device---------------
cuda:0
[2024-09-19 02:36:08,624][test.py][line:37][INFO] ---------------lossfn---------------
CrossEntropyLoss()
[2024-09-19 02:38:00,096][test.py][line:78][INFO] ---------------performance---------------
total_num:141939
error:4.267088
total_acc:0.17003078758716583
f1_score:0.0905371755361557
top5_acc:0.5031880140304565
head_acc:0.18490102829271435
medium_acc:0.1170898885887372
tail_add:0.0939907533587338

[2024-09-19 02:38:00,097][test.py][line:89][INFO] ---------------per_class_acc---------------
[0.09811855, 0.24387334, 0.0, 0.19512194, 0.12043878, 0.0, 0.02684841, 0.0, 0.14996433, 0.0, 0.040737566, 0.032258064, 0.023878021, 0.26195315, 0.13795316, 0.6818182, 0.0, 0.12376471, 0.32392704, 0.04304636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08475402, 0.04225352, 0.07636364, 0.0, 0.15528202, 0.019354839, 0.0, 0.113832854, 0.01724138, 0.0, 0.0, 0.030612245, 0.0, 0.078947365, 0.0877968, 0.0, 0.006430868, 0.027777778, 0.06666667, 0.0, 0.0, 0.11111111, 0.0, 0.04576271, 0.028571429, 0.0, 0.013333334, 0.039424613, 0.004032258, 0.056818184, 0.007575758, 0.10084783, 0.24983984, 0.19907586, 0.07724426, 0.0, 0.08737864, 0.0, 0.0, 0.0, 0.031746034, 0.018867925, 0.0, 0.07943925, 0.0, 0.034965035, 0.0, 0.06355932, 0.016129032, 0.085510686, 0.014925373, 0.007874016, 0.03448276, 0.33002833, 0.0625, 0.0, 0.11581292, 0.07669173, 0.11080332, 0.16509435, 0.0, 0.01923077, 0.0, 0.1374359, 0.0, 0.028301887, 0.021276595, 0.13837838, 0.083333336, 0.18627451, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04918033, 0.0, 0.0, 0.0, 0.0, 0.04347826, 0.014084507, 0.0, 0.0, 0.21582733, 0.07371795, 0.0, 0.0952381, 0.0, 0.031914894, 0.0, 0.017857144, 0.1724138, 0.040697675, 0.32051283, 0.31914893, 0.033333335, 0.035714287, 0.0, 0.021978023, 0.13483146, 0.01632653, 0.15714286, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04109589, 0.09565217, 0.083333336, 0.032, 0.014652015, 0.05263158, 0.2242268, 0.14248705, 0.17202798, 0.2012987, 0.39342046, 0.0, 0.0, 0.0, 0.10559006, 0.0, 0.06596306, 0.07630522, 0.0, 0.0, 0.01923077, 0.113513514, 0.2109375, 0.05620155, 0.10344828, 0.21399178, 0.14285715, 0.19480519, 0.23717949, 0.1273326, 0.0, 0.06497175, 0.028818443, 0.025, 0.0, 0.08493151, 0.0, 0.041666668, 0.10327456, 0.14285715, 0.034013607, 0.0, 0.0, 0.0, 0.25773194, 0.0, 0.0, 0.0, 0.020833334, 0.45454547, 0.0, 0.0, 0.011363637, 0.1969697, 0.22033899, 0.078947365, 0.08888889, 0.1, 0.3448276, 0.20238096, 0.5479452, 0.40298507, 0.07692308, 0.0, 0.083333336, 0.09090909, 0.296875, 0.34265736, 0.08450704, 0.0, 0.0, 0.114285715, 0.12903225, 0.1724138, 0.0, 0.21212122, 0.14285715, 0.0, 0.045454547, 0.5398773, 0.08450704, 0.0, 0.29452056, 0.36220473, 0.33333334, 0.31578946, 0.125, 0.60294116, 0.1904762, 0.5113636, 0.3888889, 0.13636364, 0.118644066]
