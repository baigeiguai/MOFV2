[2024-09-18 16:54:13,493][test.py][line:34][INFO] ---------------args---------------
Namespace(data_path='./data/Pymatgen_Wrapped/2', batch_size=128, model_path='/data/ylh/MyExps/MOFV2/checkpoints/AtL_4embed/AtL_4embed_epoch_27.pth', device='2', mode='test', top_k=5, parallel_model=False, test_name='AtLSmall_4embed', num_workers=20, log_name='log/test/test_AtLSmall_4embed_2024_09_18_16:54:08.log')
[2024-09-18 16:54:13,495][test.py][line:35][INFO] ---------------model---------------
AtLSmall(
  (embed): Embedding(8501, 3, padding_idx=0)
  (att): TransformerEncoder(
    (positionEnbeding): PositionEmbedding()
    (encoder_layers): ModuleList(
      (0-3): 4 x EncoderLayer(
        (mha): MultiHeadAttention(
          (WQ): Linear(in_features=40, out_features=40, bias=True)
          (WK): Linear(in_features=40, out_features=40, bias=True)
          (WV): Linear(in_features=40, out_features=40, bias=True)
          (scaled_dot_product_attn): ScaledDotProductAttention()
          (linear): Linear(in_features=40, out_features=40, bias=True)
        )
        (dropout1): Dropout(p=0, inplace=False)
        (layernorm1): LayerNorm((40,), eps=1e-06, elementwise_affine=True)
        (ffn): FeedForwardNetwork(
          (linear1): Linear(in_features=40, out_features=64, bias=True)
          (linear2): Linear(in_features=64, out_features=40, bias=True)
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (dropout2): Dropout(p=0, inplace=False)
        (layernorm2): LayerNorm((40,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (cls): Sequential(
    (0): Linear(in_features=40, out_features=64, bias=True)
    (1): Linear(in_features=64, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=230, bias=True)
  )
)
[2024-09-18 16:54:13,495][test.py][line:36][INFO] ---------------device---------------
cuda:2
[2024-09-18 16:54:13,495][test.py][line:37][INFO] ---------------lossfn---------------
CrossEntropyLoss()
[2024-09-18 16:55:48,381][test.py][line:78][INFO] ---------------performance---------------
total_num:141939
error:2.729517
total_acc:0.24861383438110352
f1_score:0.06607117503881454
top5_acc:0.6379571557044983
head_acc:0.29084439797825584
medium_acc:0.09277455700104686
tail_add:0.05791703445877926

[2024-09-18 16:55:48,381][test.py][line:89][INFO] ---------------per_class_acc---------------
[0.31357476, 0.34081545, 0.0, 0.37658536, 0.10208193, 0.0, 0.0012391574, 0.0, 0.31330243, 0.0, 0.00042881648, 0.001112347, 0.000863061, 0.15573858, 0.24115068, 0.6818182, 0.0, 0.002352941, 0.57521456, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00097418413, 0.0, 0.0, 0.0, 0.16542676, 0.0, 0.0, 0.02017291, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04030922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0056818184, 0.0, 0.088576525, 0.6188341, 0.47785908, 0.025052192, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013986014, 0.0, 0.05720339, 0.0, 0.095011875, 0.0, 0.0, 0.0, 0.19121814, 0.0, 0.0, 0.0066815144, 0.02406015, 0.002770083, 0.10639413, 0.0, 0.0, 0.0, 0.006153846, 0.0, 0.0, 0.0, 0.00972973, 0.0, 0.14705883, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10071942, 0.01923077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011494253, 0.0, 0.012820513, 0.27659574, 0.16666667, 0.026785715, 0.0, 0.010989011, 0.08988764, 0.028571429, 0.44285715, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0877193, 0.22938144, 0.31606218, 0.2881119, 0.19155844, 0.6946823, 0.0, 0.0, 0.0, 0.06625259, 0.0, 0.018469658, 0.02811245, 0.0, 0.0, 0.0, 0.027027028, 0.1640625, 0.036821704, 0.0, 0.17695473, 0.020408163, 0.14025974, 0.38141027, 0.18770581, 0.0, 0.11581921, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007556675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2783505, 0.0, 0.0, 0.0, 0.0, 0.045454547, 0.0, 0.0, 0.0, 0.24242425, 0.5762712, 0.0, 0.23333333, 0.0, 0.22413793, 0.0952381, 0.7808219, 0.35820895, 0.0, 0.0, 0.0, 0.018181818, 0.140625, 0.56876457, 0.014084507, 0.0, 0.0, 0.028571429, 0.16129032, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5092025, 0.14084508, 0.0, 0.15068494, 0.4566929, 0.46153846, 0.0, 0.0, 0.59117645, 0.023809524, 0.06818182, 0.3472222, 0.0, 0.084745765]
