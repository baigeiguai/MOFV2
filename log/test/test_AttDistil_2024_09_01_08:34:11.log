[2024-09-01 08:34:15,908][test.py][line:34][INFO] ---------------args---------------
Namespace(data_path='./data/Pymatgen_Wrapped/0', batch_size=32, model_path='/data/ylh/MyExps/MOFV2/AttDistil/AttDistil_epoch_98.pth', device='6', mode='test', top_k=5, parallel_model=False, test_name='AttDistil', num_workers=20, log_name='log/test/test_AttDistil_2024_09_01_08:34:11.log')
[2024-09-01 08:34:15,910][test.py][line:35][INFO] ---------------model---------------
AttDistil(
  (embed): Embedding(8500, 16)
  (encoders): ModuleList(
    (0-23): 24 x EncoderLayer(
      (mha): MultiHeadAttention(
        (WQ): Linear(in_features=160, out_features=160, bias=True)
        (WK): Linear(in_features=160, out_features=160, bias=True)
        (WV): Linear(in_features=160, out_features=160, bias=True)
        (scaled_dot_product_attn): ScaledDotProductAttention()
        (linear): Linear(in_features=160, out_features=160, bias=True)
      )
      (dropout1): Dropout(p=0, inplace=False)
      (layernorm1): LayerNorm((160,), eps=1e-06, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (linear1): Linear(in_features=160, out_features=256, bias=True)
        (linear2): Linear(in_features=256, out_features=160, bias=True)
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (dropout2): Dropout(p=0, inplace=False)
      (layernorm2): LayerNorm((160,), eps=1e-06, elementwise_affine=True)
    )
  )
  (to_features): Linear(in_features=136000, out_features=1024, bias=True)
  (to_cls): Linear(in_features=1024, out_features=230, bias=True)
)
[2024-09-01 08:34:15,910][test.py][line:36][INFO] ---------------device---------------
cuda:6
[2024-09-01 08:34:15,910][test.py][line:37][INFO] ---------------lossfn---------------
CrossEntropyLoss()
[2024-09-01 08:48:09,483][test.py][line:78][INFO] ---------------performance---------------
total_num:142729
error:6.751661
total_acc:0.14638930559158325
f1_score:0.08386801183223724
top5_acc:0.4471971392631531
head_acc:0.16903820509200423
medium_acc:0.05978461207569046
tail_add:0.05995221515769107

[2024-09-01 08:48:09,484][test.py][line:89][INFO] ---------------per_class_acc---------------
[0.13744265, 0.22111511, 0.013157895, 0.21593608, 0.084278405, 0.055555556, 0.026715988, 0.0056818184, 0.17575973, 0.0, 0.043904517, 0.021017699, 0.035440885, 0.18724804, 0.17271377, 0.65217394, 0.02631579, 0.024674116, 0.3061334, 0.018599562, 0.0, 0.0, 0.023622047, 0.0, 0.0, 0.065789476, 0.05882353, 0.0, 0.053002674, 0.0, 0.032608695, 0.0, 0.14781787, 0.006451613, 0.2, 0.023054754, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.034197465, 0.0, 0.009584664, 0.027777778, 0.06666667, 0.0, 0.0, 0.0, 0.0, 0.020304568, 0.014084507, 0.00390625, 0.013245033, 0.01746956, 0.008048289, 0.025423728, 0.0, 0.07312736, 0.19263516, 0.17307693, 0.031185031, 0.0, 0.019417476, 0.0, 0.0, 0.0, 0.07936508, 0.022108844, 0.0, 0.046511628, 0.025477707, 0.027972028, 0.0, 0.014830508, 0.0, 0.011876484, 0.0, 0.007874016, 0.0, 0.11756374, 0.0, 0.0, 0.04454343, 0.03153153, 0.077562325, 0.04908616, 0.0, 0.03846154, 0.0, 0.017382413, 0.0, 0.03773585, 0.0, 0.026939655, 0.0, 0.27450982, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04347826, 0.028169014, 0.0, 0.0, 0.15107913, 0.0288, 0.0, 0.0952381, 0.030303031, 0.07446808, 0.0625, 0.0, 0.03448276, 0.026086956, 0.16666667, 0.14893617, 0.13333334, 0.05357143, 0.0, 0.13186814, 0.11235955, 0.05284553, 0.31428573, 0.0, 0.0, 0.071428575, 0.0, 0.07608695, 0.01754386, 0.0, 0.034782607, 0.055555556, 0.055555556, 0.010989011, 0.00877193, 0.085051544, 0.08549223, 0.12849163, 0.056726094, 0.4157101, 0.0, 0.0, 0.0, 0.028985508, 0.0, 0.02356021, 0.0562249, 0.0, 0.0, 0.01923077, 0.016216217, 0.1328125, 0.019379845, 0.03448276, 0.16460906, 0.08163265, 0.12207792, 0.25559106, 0.05592105, 0.0, 0.050704226, 0.028818443, 0.0, 0.0, 0.06284153, 0.0625, 0.0, 0.07788945, 0.0, 0.06802721, 0.016, 0.025, 0.04, 0.17525773, 0.0, 0.0, 0.0, 0.0, 0.13636364, 0.0, 0.0, 0.0, 0.15151516, 0.050847456, 0.02631579, 0.08287293, 0.0, 0.13793103, 0.05952381, 0.24657534, 0.3880597, 0.15384616, 0.0, 0.0, 0.036363635, 0.125, 0.11188811, 0.0, 0.07692308, 0.0, 0.028571429, 0.22580644, 0.27586207, 0.0, 0.030303031, 0.071428575, 0.13636364, 0.045454547, 0.398773, 0.028169014, 0.0, 0.07534247, 0.17322835, 0.17948718, 0.05263158, 0.125, 0.5382353, 0.071428575, 0.2840909, 0.125, 0.13636364, 0.0]
