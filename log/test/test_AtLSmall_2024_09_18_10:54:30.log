[2024-09-18 10:54:38,672][test.py][line:34][INFO] ---------------args---------------
Namespace(data_path='./data/Pymatgen_Wrapped/2', batch_size=128, model_path='/data/ylh/MyExps/MOFV2/checkpoints/AtLSmall/AtLSmall_epoch_198.pth', device='3', mode='test', top_k=5, parallel_model=False, test_name='AtLSmall', num_workers=20, log_name='log/test/test_AtLSmall_2024_09_18_10:54:30.log')
[2024-09-18 10:54:38,674][test.py][line:35][INFO] ---------------model---------------
AtLSmall(
  (embed): Embedding(8501, 15, padding_idx=0)
  (att): TransformerEncoder(
    (positionEnbeding): PositionEmbedding()
    (encoder_layers): ModuleList(
      (0-7): 8 x EncoderLayer(
        (mha): MultiHeadAttention(
          (WQ): Linear(in_features=160, out_features=160, bias=True)
          (WK): Linear(in_features=160, out_features=160, bias=True)
          (WV): Linear(in_features=160, out_features=160, bias=True)
          (scaled_dot_product_attn): ScaledDotProductAttention()
          (linear): Linear(in_features=160, out_features=160, bias=True)
        )
        (dropout1): Dropout(p=0.35, inplace=False)
        (layernorm1): LayerNorm((160,), eps=1e-06, elementwise_affine=True)
        (ffn): FeedForwardNetwork(
          (linear1): Linear(in_features=160, out_features=512, bias=True)
          (linear2): Linear(in_features=512, out_features=160, bias=True)
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (dropout2): Dropout(p=0.35, inplace=False)
        (layernorm2): LayerNorm((160,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (cls): Sequential(
    (0): Linear(in_features=160, out_features=256, bias=True)
    (1): Linear(in_features=256, out_features=230, bias=True)
  )
)
[2024-09-18 10:54:38,674][test.py][line:36][INFO] ---------------device---------------
cuda:3
[2024-09-18 10:54:38,674][test.py][line:37][INFO] ---------------lossfn---------------
CrossEntropyLoss()
[2024-09-18 10:59:26,027][test.py][line:78][INFO] ---------------performance---------------
total_num:141939
error:12.325790
total_acc:0.2928018271923065
f1_score:0.170448899269104
top5_acc:0.6632497310638428
head_acc:0.3204429308695128
medium_acc:0.19042789354230275
tail_add:0.1699923043782712

[2024-09-18 10:59:26,028][test.py][line:89][INFO] ---------------per_class_acc---------------
[0.3564637, 0.30893517, 0.026666667, 0.2801084, 0.2274457, 0.055555556, 0.11400248, 0.07954545, 0.33933666, 0.016666668, 0.16252144, 0.14015573, 0.11967779, 0.36677718, 0.33279863, 0.6818182, 0.0, 0.22117648, 0.5534335, 0.10375276, 0.0, 0.0, 0.048, 0.0, 0.0, 0.09210526, 0.0, 0.0, 0.10618607, 0.04225352, 0.08, 0.022222223, 0.26078722, 0.11612903, 0.2, 0.12680115, 0.01724138, 0.045454547, 0.0, 0.1122449, 0.0, 0.23684211, 0.1750414, 0.054054055, 0.03858521, 0.11111111, 0.2, 0.027027028, 0.0, 0.12962963, 0.0, 0.096610166, 0.042857144, 0.027559055, 0.02, 0.061800744, 0.050403226, 0.09375, 0.022727273, 0.20727354, 0.49188554, 0.30015403, 0.12526096, 0.0, 0.18446602, 0.06153846, 0.0, 0.0, 0.14285715, 0.15951973, 0.014084507, 0.13551402, 0.044871796, 0.062937066, 0.03448276, 0.16101696, 0.08064516, 0.11401425, 0.07462686, 0.09448819, 0.02586207, 0.28895184, 0.0, 0.020833334, 0.1492205, 0.13383459, 0.21883656, 0.28197065, 0.0, 0.115384616, 0.022727273, 0.16820513, 0.0, 0.08490566, 0.021276595, 0.16432433, 0.0625, 0.3627451, 0.5, 0.0, 0.0, 0.0, 0.0, 0.18032786, 1.0, 0.018867925, 0.083333336, 0.15789473, 0.04347826, 0.07511737, 0.0, 0.0, 0.29496402, 0.118589744, 0.0, 0.0952381, 0.18181819, 0.095744684, 0.0625, 0.08928572, 0.12643678, 0.11627907, 0.35897437, 0.34042552, 0.2, 0.08928572, 0.0, 0.26373628, 0.20224719, 0.14693877, 0.35714287, 0.1, 0.041666668, 0.035714287, 0.04, 0.20879121, 0.07017544, 0.15068494, 0.13913043, 0.2361111, 0.168, 0.09157509, 0.15789473, 0.2371134, 0.20207255, 0.26013985, 0.3685065, 0.5081118, 0.5, 0.074074075, 0.0, 0.20082815, 0.0, 0.09762533, 0.17670682, 0.0, 0.11111111, 0.057692308, 0.11891892, 0.2734375, 0.18604651, 0.1724138, 0.30864197, 0.2857143, 0.25454545, 0.36858973, 0.25905597, 0.07692308, 0.14689265, 0.10086455, 0.025, 0.0, 0.1589041, 0.125, 0.041666668, 0.2141058, 0.14285715, 0.11564626, 0.032, 0.025, 0.04, 0.371134, 0.0, 0.0, 0.05263158, 0.083333336, 0.45454547, 0.125, 0.13636364, 0.0, 0.33333334, 0.42372882, 0.15789473, 0.24444444, 0.0, 0.5862069, 0.35714287, 0.7328767, 0.58208954, 0.23076923, 0.16, 0.083333336, 0.16363636, 0.40625, 0.53613055, 0.23943663, 0.30769232, 0.0, 0.17142858, 0.2580645, 0.51724136, 0.041666668, 0.3030303, 0.14285715, 0.09090909, 0.13636364, 0.5460123, 0.29577464, 0.104166664, 0.5205479, 0.56692916, 0.55128205, 0.28947368, 0.375, 0.7558824, 0.33333334, 0.4318182, 0.41666666, 0.18181819, 0.13559322]
