[2024-09-18 14:42:46,781][test.py][line:34][INFO] ---------------args---------------
Namespace(data_path='./data/Pymatgen_Wrapped/2', batch_size=128, model_path='/data/ylh/MyExps/MOFV2/checkpoints/AtLSmall_2layer/AtLSmall_2layer_epoch_13.pth', device='7', mode='test', top_k=5, parallel_model=False, test_name='AtLSmall_2Layer', num_workers=20, log_name='log/test/test_AtLSmall_2Layer_2024_09_18_14:42:41.log')
[2024-09-18 14:42:46,782][test.py][line:35][INFO] ---------------model---------------
AtLSmall(
  (embed): Embedding(8501, 31, padding_idx=0)
  (att): TransformerEncoder(
    (positionEnbeding): PositionEmbedding()
    (encoder_layers): ModuleList(
      (0-1): 2 x EncoderLayer(
        (mha): MultiHeadAttention(
          (WQ): Linear(in_features=320, out_features=320, bias=True)
          (WK): Linear(in_features=320, out_features=320, bias=True)
          (WV): Linear(in_features=320, out_features=320, bias=True)
          (scaled_dot_product_attn): ScaledDotProductAttention()
          (linear): Linear(in_features=320, out_features=320, bias=True)
        )
        (dropout1): Dropout(p=0, inplace=False)
        (layernorm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (ffn): FeedForwardNetwork(
          (linear1): Linear(in_features=320, out_features=512, bias=True)
          (linear2): Linear(in_features=512, out_features=320, bias=True)
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (dropout2): Dropout(p=0, inplace=False)
        (layernorm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (cls): Sequential(
    (0): Linear(in_features=320, out_features=256, bias=True)
    (1): Linear(in_features=256, out_features=230, bias=True)
  )
)
[2024-09-18 14:42:46,783][test.py][line:36][INFO] ---------------device---------------
cuda:7
[2024-09-18 14:42:46,783][test.py][line:37][INFO] ---------------lossfn---------------
CrossEntropyLoss()
[2024-09-18 14:45:22,538][test.py][line:78][INFO] ---------------performance---------------
total_num:141939
error:2.877790
total_acc:0.27211689949035645
f1_score:0.10525300353765488
top5_acc:0.6604597568511963
head_acc:0.3091140753527759
medium_acc:0.13972651244252024
tail_add:0.08732655602312968

[2024-09-18 14:45:22,539][test.py][line:89][INFO] ---------------per_class_acc---------------
[0.19805786, 0.42279333, 0.0, 0.31479675, 0.29191852, 0.0, 0.031391986, 0.08522727, 0.43705422, 0.0, 0.052744426, 0.14349277, 0.007479862, 0.12717938, 0.36883757, 0.6818182, 0.0, 0.18305883, 0.5393777, 0.02759382, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.063565515, 0.028169014, 0.02909091, 0.0, 0.22967672, 0.019354839, 0.0, 0.12247839, 0.01724138, 0.0, 0.0, 0.020408163, 0.0, 0.0, 0.090005524, 0.0, 0.012861736, 0.027777778, 0.0, 0.0, 0.0, 0.018518519, 0.0, 0.010169491, 0.0, 0.0, 0.006666667, 0.00053276506, 0.0, 0.025568182, 0.0, 0.12650603, 0.53171045, 0.38390452, 0.073068894, 0.0, 0.048543688, 0.0, 0.0, 0.0, 0.06349207, 0.022298457, 0.04225352, 0.037383176, 0.0, 0.062937066, 0.0, 0.1970339, 0.016129032, 0.014251782, 0.0, 0.0, 0.05172414, 0.29886687, 0.125, 0.0, 0.08908686, 0.08571429, 0.083102494, 0.2562893, 0.0, 0.01923077, 0.022727273, 0.23179488, 0.0, 0.028301887, 0.0, 0.018378379, 0.0, 0.2647059, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20143884, 0.07532051, 0.0, 0.04761905, 0.0, 0.05319149, 0.0, 0.0, 0.04597701, 0.031976745, 0.06410257, 0.4680851, 0.06666667, 0.04464286, 0.0, 0.021978023, 0.08988764, 0.093877554, 0.34285715, 0.0, 0.0, 0.035714287, 0.0, 0.021978023, 0.03508772, 0.08219178, 0.026086956, 0.013888889, 0.12, 0.08424909, 0.096491225, 0.21907216, 0.19689119, 0.2769231, 0.26461038, 0.6300135, 0.16666667, 0.0, 0.0, 0.049689442, 0.0, 0.04221636, 0.072289154, 0.0, 0.0, 0.01923077, 0.17297298, 0.125, 0.1007752, 0.03448276, 0.25514403, 0.24489796, 0.24155845, 0.34935898, 0.21734358, 0.0, 0.20903955, 0.017291067, 0.0, 0.0, 0.03287671, 0.0, 0.0, 0.1209068, 0.0, 0.013605442, 0.008, 0.0, 0.0, 0.20618556, 0.0, 0.0, 0.0, 0.0625, 0.45454547, 0.0, 0.0, 0.0, 0.18181819, 0.47457626, 0.0, 0.15, 0.1, 0.39655173, 0.083333336, 0.739726, 0.4477612, 0.07692308, 0.12, 0.0, 0.09090909, 0.28125, 0.55710953, 0.16901408, 0.0, 0.0, 0.057142857, 0.22580644, 0.27586207, 0.0, 0.121212125, 0.0, 0.045454547, 0.0, 0.5276074, 0.2535211, 0.0, 0.32876712, 0.37007874, 0.43589744, 0.02631579, 0.1875, 0.65588236, 0.2857143, 0.1590909, 0.3472222, 0.22727273, 0.15254237]
