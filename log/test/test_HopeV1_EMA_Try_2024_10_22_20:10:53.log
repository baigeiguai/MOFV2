[2024-10-22 20:11:00,429][test.py][line:34][INFO] ---------------args---------------
Namespace(data_path='./data/Pymatgen_Wrapped/0', batch_size=128, model_path='/data/ylh/MyExps/MOFV2/checkpoints/HopeV1_EMA_Try/HopeV1_EMA_Try_epoch_54.pth', device='2', mode='test', top_k=5, parallel_model=False, test_name='HopeV1_EMA_Try', num_workers=20, log_name='log/test/test_HopeV1_EMA_Try_2024_10_22_20:10:53.log')
[2024-10-22 20:11:00,432][test.py][line:35][INFO] ---------------model---------------
HopeV1(
  (conv_module): ResTcn(
    (intensity_norm): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (TCN): Sequential(
      (0): ResBlock1D(
        (pre): Conv1d(2, 32, kernel_size=(1,), stride=(1,), bias=False)
        (conv): Sequential(
          (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (2): ResBlock1D(
        (pre): Identity()
        (conv): Sequential(
          (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (4): ResBlock1D(
        (pre): Conv1d(32, 64, kernel_size=(1,), stride=(1,), bias=False)
        (conv): Sequential(
          (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (6): ResBlock1D(
        (pre): Conv1d(64, 128, kernel_size=(1,), stride=(1,), bias=False)
        (conv): Sequential(
          (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (7): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (8): ResBlock1D(
        (pre): Identity()
        (conv): Sequential(
          (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (9): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
      (10): ResBlock1D(
        (pre): Conv1d(128, 256, kernel_size=(1,), stride=(1,), bias=False)
        (conv): Sequential(
          (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (11): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
      (12): ResBlock1D(
        (pre): Identity()
        (conv): Sequential(
          (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (13): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
      (14): ResBlock1D(
        (pre): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
        (conv): Sequential(
          (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (15): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
      (16): ResBlock1D(
        (pre): Identity()
        (conv): Sequential(
          (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (17): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
      (18): ResBlock1D(
        (pre): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
        (conv): Sequential(
          (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (19): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
      (20): Dropout(p=0.25, inplace=False)
      (21): ResBlock1D(
        (pre): Identity()
        (conv): Sequential(
          (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (22): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
      (23): Dropout(p=0.25, inplace=False)
      (24): ResBlock1D(
        (pre): Identity()
        (conv): Sequential(
          (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (25): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
      (26): Dropout(p=0.25, inplace=False)
      (27): ResBlock1D(
        (pre): Identity()
        (conv): Sequential(
          (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (28): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
      (29): Flatten(start_dim=1, end_dim=-1)
    )
  )
  (att): AttentionModule(
    (embed): Embedding(850, 15)
    (patch_conv): PatchConvModule(
      (conv): Sequential(
        (0): Conv2d(1, 2, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        (1): AvgPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0)
        (2): Conv2d(2, 4, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        (3): AvgPool2d(kernel_size=(1, 2), stride=(1, 2), padding=(0, 1))
        (4): Conv2d(4, 8, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        (5): AvgPool2d(kernel_size=(1, 2), stride=(1, 2), padding=(0, 1))
        (6): Conv2d(8, 16, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        (7): AvgPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0)
      )
    )
    (att): TransformerEncoder(
      (positionEnbeding): PositionEmbedding()
      (encoder_layers): ModuleList(
        (0-7): 8 x EncoderLayer(
          (mha): MultiHeadAttention(
            (WQ): Linear(in_features=32, out_features=32, bias=True)
            (WK): Linear(in_features=32, out_features=32, bias=True)
            (WV): Linear(in_features=32, out_features=32, bias=True)
            (scaled_dot_product_attn): ScaledDotProductAttention()
            (linear): Linear(in_features=32, out_features=32, bias=True)
          )
          (dropout1): Dropout(p=0.25, inplace=False)
          (layernorm1): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (linear1): Linear(in_features=32, out_features=256, bias=True)
            (linear2): Linear(in_features=256, out_features=32, bias=True)
            (relu): LeakyReLU(negative_slope=0.01)
          )
          (dropout2): Dropout(p=0.25, inplace=False)
          (layernorm2): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
  )
  (cls_sp): Sequential(
    (0): Linear(in_features=1056, out_features=230, bias=True)
  )
  (cls_cs): Sequential(
    (0): Linear(in_features=1056, out_features=7, bias=True)
  )
  (cls_lt): Sequential(
    (0): Linear(in_features=1056, out_features=6, bias=True)
  )
)
[2024-10-22 20:11:00,432][test.py][line:36][INFO] ---------------device---------------
cuda:2
[2024-10-22 20:11:00,432][test.py][line:37][INFO] ---------------lossfn---------------
CrossEntropyLoss()
[2024-10-22 20:15:38,647][test.py][line:88][INFO] ---------------performance---------------
total_num:142729
error:1.655302
total_acc:0.6469953656196594
f1_score:0.4594259262084961
top5_acc:0.9283397197723389
head_acc:0.6996264675630549
medium_acc:0.6132611771491926
tail_add:0.5066008333407203
crystal_system_acc:0.8729971051216125
lattice_type_acc:0.9140468835830688

[2024-10-22 20:15:38,648][test.py][line:101][INFO] ---------------per_class_acc---------------
[0.839617, 0.5923256, 0.118421055, 0.6381991, 0.6077385, 0.33333334, 0.62885326, 0.39204547, 0.73502755, 0.19672132, 0.4428815, 0.30641592, 0.37142047, 0.73488224, 0.50530446, 0.6956522, 0.15789473, 0.43901303, 0.7149904, 0.32056892, 0.125, 0.10526316, 0.42519686, 0.14705883, 0.0, 0.25, 0.05882353, 0.11111111, 0.58473134, 0.3802817, 0.4456522, 0.12222222, 0.8151601, 0.2580645, 0.2, 0.5489914, 0.42372882, 0.4090909, 0.0, 0.2755102, 0.0, 0.57894737, 0.7346939, 0.3783784, 0.53035146, 0.30555555, 0.06666667, 0.45945945, 0.4, 0.24074075, 0.23076923, 0.42978004, 0.47887325, 0.33203125, 0.14569536, 0.6590789, 0.21529175, 0.27966103, 0.2932331, 0.60657924, 0.9740315, 0.43096155, 0.46777546, 0.0, 0.5048544, 0.22727273, 0.0, 0.0, 0.3968254, 0.6972789, 0.4027778, 0.4511628, 0.26751593, 0.5034965, 0.13793103, 0.7457627, 0.24193548, 0.14014252, 0.14925373, 0.5984252, 0.5, 0.6827195, 0.125, 0.41666666, 0.7349666, 0.8168168, 0.7036011, 0.8981723, 0.0, 0.30769232, 0.24444444, 0.24233128, 0.0, 0.2735849, 0.04255319, 0.64224136, 0.22916667, 0.61764705, 0.0, 0.33333334, 0.0, 0.07692308, 0.4117647, 0.7704918, 1.0, 0.43396226, 0.083333336, 0.42105263, 0.2173913, 0.69953054, 0.0, 0.1764706, 0.7410072, 0.8128, 0.0, 0.23809524, 0.45454547, 0.5, 0.0625, 0.5, 0.51724136, 0.68115944, 0.75641024, 0.78723407, 0.8666667, 0.77678573, 0.2857143, 0.34065935, 0.6516854, 0.8089431, 0.71428573, 0.2, 0.16, 0.4642857, 0.32, 0.5869565, 0.5614035, 0.4520548, 0.09565217, 0.625, 0.47619048, 0.62637365, 0.42105263, 0.34536082, 0.48445597, 0.49581006, 0.6969206, 0.9257259, 0.33333334, 0.09259259, 0.0, 0.3830228, 0.09090909, 0.34554973, 0.19277108, 0.0, 0.11111111, 0.5, 0.64324325, 0.640625, 0.88372093, 0.5862069, 0.5555556, 0.40816328, 0.74545455, 0.37060702, 0.7105263, 0.0, 0.47323942, 0.351585, 0.075, 0.0, 0.58743167, 0.0625, 0.375, 0.74371856, 0.26666668, 0.40136054, 0.36, 0.275, 0.24, 0.49484536, 0.0, 0.2857143, 0.10526316, 0.27083334, 0.54545456, 0.125, 0.22727273, 0.3409091, 0.5151515, 0.8305085, 0.55263156, 0.6077348, 0.3, 0.6896552, 0.23809524, 0.82191783, 0.67164177, 0.0, 0.32, 0.0, 0.90909094, 0.84375, 0.995338, 0.91549295, 0.30769232, 0.33333334, 0.17142858, 0.67741936, 0.37931034, 0.375, 0.8484849, 0.85714287, 0.36363637, 0.0, 0.607362, 0.90140843, 0.9166667, 0.9931507, 0.8976378, 0.98717946, 0.7894737, 0.8125, 0.9823529, 0.6904762, 0.92045456, 0.9722222, 0.4090909, 1.0]
