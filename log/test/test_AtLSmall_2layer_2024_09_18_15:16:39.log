[2024-09-18 15:16:45,112][test.py][line:34][INFO] ---------------args---------------
Namespace(data_path='./data/Pymatgen_Wrapped/2', batch_size=128, model_path='/data/ylh/MyExps/MOFV2/checkpoints/AtLSmall_2layer/AtLSmall_2layer_epoch_21.pth', device='1', mode='test', top_k=5, parallel_model=False, test_name='AtLSmall_2layer', num_workers=20, log_name='log/test/test_AtLSmall_2layer_2024_09_18_15:16:39.log')
[2024-09-18 15:16:45,113][test.py][line:35][INFO] ---------------model---------------
AtLSmall(
  (embed): Embedding(8501, 31, padding_idx=0)
  (att): TransformerEncoder(
    (positionEnbeding): PositionEmbedding()
    (encoder_layers): ModuleList(
      (0-1): 2 x EncoderLayer(
        (mha): MultiHeadAttention(
          (WQ): Linear(in_features=320, out_features=320, bias=True)
          (WK): Linear(in_features=320, out_features=320, bias=True)
          (WV): Linear(in_features=320, out_features=320, bias=True)
          (scaled_dot_product_attn): ScaledDotProductAttention()
          (linear): Linear(in_features=320, out_features=320, bias=True)
        )
        (dropout1): Dropout(p=0, inplace=False)
        (layernorm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (ffn): FeedForwardNetwork(
          (linear1): Linear(in_features=320, out_features=512, bias=True)
          (linear2): Linear(in_features=512, out_features=320, bias=True)
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (dropout2): Dropout(p=0, inplace=False)
        (layernorm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (cls): Sequential(
    (0): Linear(in_features=320, out_features=256, bias=True)
    (1): Linear(in_features=256, out_features=230, bias=True)
  )
)
[2024-09-18 15:16:45,114][test.py][line:36][INFO] ---------------device---------------
cuda:1
[2024-09-18 15:16:45,114][test.py][line:37][INFO] ---------------lossfn---------------
CrossEntropyLoss()
[2024-09-18 15:19:10,948][test.py][line:78][INFO] ---------------performance---------------
total_num:141939
error:3.907587
total_acc:0.25695544481277466
f1_score:0.10417041182518005
top5_acc:0.6268889904022217
head_acc:0.2932314344890603
medium_acc:0.12696064047998654
tail_add:0.07985320341394757

[2024-09-18 15:19:10,948][test.py][line:89][INFO] ---------------per_class_acc---------------
[0.40643334, 0.3678161, 0.0, 0.33777776, 0.19901499, 0.0, 0.088393226, 0.045454547, 0.27942225, 0.0, 0.0990566, 0.10901001, 0.059838895, 0.2819553, 0.2996471, 0.0, 0.02631579, 0.20470588, 0.45976394, 0.054083884, 0.0, 0.0, 0.016, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07160253, 0.014084507, 0.054545455, 0.0, 0.23738672, 0.038709678, 0.0, 0.12536024, 0.03448276, 0.0, 0.0, 0.040816326, 0.0, 0.02631579, 0.09331861, 0.0, 0.01607717, 0.013888889, 0.13333334, 0.0, 0.0, 0.0, 0.0, 0.02881356, 0.0, 0.003937008, 0.006666667, 0.025039958, 0.002016129, 0.022727273, 0.007575758, 0.16175814, 0.3896007, 0.31786677, 0.08559499, 0.0, 0.029126214, 0.0, 0.0, 0.0, 0.031746034, 0.05317324, 0.028169014, 0.03271028, 0.0, 0.055944055, 0.0, 0.14830509, 0.016129032, 0.03087886, 0.0074626864, 0.062992126, 0.10344828, 0.20538244, 0.0, 0.0, 0.18262807, 0.10977443, 0.08033241, 0.3071279, 0.0, 0.09615385, 0.022727273, 0.11076923, 0.0, 0.047169812, 0.0, 0.12648648, 0.020833334, 0.2254902, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 1.0, 0.018867925, 0.083333336, 0.05263158, 0.04347826, 0.018779343, 0.0, 0.0, 0.15107913, 0.07532051, 0.0, 0.04761905, 0.060606062, 0.031914894, 0.0, 0.017857144, 0.06896552, 0.06104651, 0.15384616, 0.19148937, 0.1, 0.026785715, 0.0, 0.07692308, 0.07865169, 0.08163265, 0.114285715, 0.0, 0.041666668, 0.0, 0.0, 0.010989011, 0.07017544, 0.12328767, 0.026086956, 0.055555556, 0.136, 0.07326008, 0.01754386, 0.29639176, 0.062176164, 0.14545454, 0.2159091, 0.51892745, 0.16666667, 0.0, 0.0, 0.072463766, 0.0, 0.10817942, 0.10843374, 0.0, 0.11111111, 0.01923077, 0.15135135, 0.1953125, 0.10852713, 0.06896552, 0.25514403, 0.1632653, 0.22077923, 0.26602563, 0.18770581, 0.0, 0.19774011, 0.034582134, 0.0, 0.0, 0.024657534, 0.0, 0.083333336, 0.1536524, 0.0, 0.04761905, 0.024, 0.0, 0.0, 0.21649484, 0.0, 0.0, 0.0, 0.041666668, 0.18181819, 0.0, 0.0, 0.011363637, 0.13636364, 0.37288135, 0.0, 0.15, 0.1, 0.3275862, 0.13095239, 0.6130137, 0.43283582, 0.07692308, 0.12, 0.083333336, 0.10909091, 0.359375, 0.44755244, 0.1971831, 0.0, 0.0, 0.057142857, 0.19354838, 0.2413793, 0.0, 0.15151516, 0.0, 0.0, 0.0, 0.47239265, 0.22535211, 0.0, 0.29452056, 0.26771653, 0.43589744, 0.02631579, 0.0, 0.47058824, 0.16666667, 0.125, 0.2638889, 0.18181819, 0.16949153]
