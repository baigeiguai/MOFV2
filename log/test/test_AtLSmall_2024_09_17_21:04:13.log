[2024-09-17 21:04:19,358][test.py][line:34][INFO] ---------------args---------------
Namespace(data_path='./data/Pymatgen_Wrapped/2', batch_size=128, model_path='/data/ylh/MyExps/MOFV2/checkpoints/AtLSmall/AtLSmall_epoch_25.pth', device='0', mode='test', top_k=5, parallel_model=False, test_name='AtLSmall', num_workers=20, log_name='log/test/test_AtLSmall_2024_09_17_21:04:13.log')
[2024-09-17 21:04:19,360][test.py][line:35][INFO] ---------------model---------------
AtLSmall(
  (embed): Embedding(8501, 15, padding_idx=0)
  (att): TransformerEncoder(
    (positionEnbeding): PositionEmbedding()
    (encoder_layers): ModuleList(
      (0-7): 8 x EncoderLayer(
        (mha): MultiHeadAttention(
          (WQ): Linear(in_features=160, out_features=160, bias=True)
          (WK): Linear(in_features=160, out_features=160, bias=True)
          (WV): Linear(in_features=160, out_features=160, bias=True)
          (scaled_dot_product_attn): ScaledDotProductAttention()
          (linear): Linear(in_features=160, out_features=160, bias=True)
        )
        (dropout1): Dropout(p=0.35, inplace=False)
        (layernorm1): LayerNorm((160,), eps=1e-06, elementwise_affine=True)
        (ffn): FeedForwardNetwork(
          (linear1): Linear(in_features=160, out_features=512, bias=True)
          (linear2): Linear(in_features=512, out_features=160, bias=True)
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (dropout2): Dropout(p=0.35, inplace=False)
        (layernorm2): LayerNorm((160,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (cls): Sequential(
    (0): Linear(in_features=160, out_features=256, bias=True)
    (1): Linear(in_features=256, out_features=230, bias=True)
  )
)
[2024-09-17 21:04:19,360][test.py][line:36][INFO] ---------------device---------------
cuda:0
[2024-09-17 21:04:19,360][test.py][line:37][INFO] ---------------lossfn---------------
CrossEntropyLoss()
[2024-09-17 21:08:18,241][test.py][line:78][INFO] ---------------performance---------------
total_num:141939
error:3.649719
total_acc:0.27354708313941956
f1_score:0.13459226489067078
top5_acc:0.6488280296325684
head_acc:0.30233646390568025
medium_acc:0.17068906634722839
tail_add:0.12433286209862789

[2024-09-17 21:08:18,242][test.py][line:89][INFO] ---------------per_class_acc---------------
[0.36415133, 0.2180655, 0.0, 0.29615176, 0.21423775, 0.055555556, 0.049979348, 0.09659091, 0.32257488, 0.016666668, 0.13293311, 0.1512792, 0.050345223, 0.27863944, 0.30659822, 0.0, 0.0, 0.21035294, 0.5550429, 0.04304636, 0.0, 0.0, 0.048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028007794, 0.04225352, 0.02909091, 0.0, 0.25578249, 0.058064517, 0.0, 0.096541785, 0.01724138, 0.13636364, 0.0, 0.08163265, 0.0, 0.13157895, 0.17448923, 0.0, 0.01607717, 0.055555556, 0.2, 0.0, 0.0, 0.12962963, 0.03846154, 0.11016949, 0.0, 0.003937008, 0.013333334, 0.021843366, 0.008064516, 0.11647727, 0.0, 0.2614904, 0.5267991, 0.29745862, 0.08350731, 0.0, 0.097087376, 0.0, 0.0, 0.0, 0.0952381, 0.19725558, 0.028169014, 0.05140187, 0.0, 0.07692308, 0.03448276, 0.15254237, 0.048387095, 0.07363421, 0.0074626864, 0.015748031, 0.15517241, 0.33569404, 0.0, 0.0, 0.042316258, 0.12631579, 0.23822714, 0.2725367, 0.0, 0.13461539, 0.045454547, 0.14564103, 0.0, 0.03773585, 0.0, 0.1372973, 0.0, 0.39215687, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04918033, 1.0, 0.0, 0.0, 0.05263158, 0.04347826, 0.0046948357, 0.0, 0.0, 0.22302158, 0.086538464, 0.0, 0.0952381, 0.18181819, 0.04255319, 0.0, 0.10714286, 0.057471264, 0.072674416, 0.20512821, 0.31914893, 0.3, 0.035714287, 0.0, 0.054945055, 0.14606741, 0.12653062, 0.34285715, 0.0, 0.0, 0.0, 0.04, 0.054945055, 0.03508772, 0.1780822, 0.069565214, 0.15277778, 0.216, 0.13919415, 0.14035088, 0.4072165, 0.06476684, 0.1958042, 0.38474026, 0.5590356, 0.33333334, 0.055555556, 0.0, 0.13871635, 0.0, 0.113456465, 0.104417674, 0.0, 0.0, 0.057692308, 0.1027027, 0.2578125, 0.12209302, 0.13793103, 0.30041152, 0.2244898, 0.2, 0.35897437, 0.24698134, 0.0, 0.13559322, 0.12103746, 0.025, 0.0, 0.090410955, 0.0, 0.041666668, 0.19395466, 0.071428575, 0.013605442, 0.008, 0.05, 0.0, 0.28865978, 0.0, 0.0, 0.05263158, 0.041666668, 0.22727273, 0.0, 0.045454547, 0.0, 0.3939394, 0.3898305, 0.10526316, 0.1388889, 0.0, 0.5344828, 0.3690476, 0.7328767, 0.47761193, 0.0, 0.16, 0.083333336, 0.12727273, 0.296875, 0.6270396, 0.14084508, 0.15384616, 0.0, 0.08571429, 0.2580645, 0.41379312, 0.125, 0.24242425, 0.14285715, 0.0, 0.0, 0.5582822, 0.29577464, 0.104166664, 0.29452056, 0.5590551, 0.4871795, 0.18421052, 0.0, 0.7735294, 0.3809524, 0.22727273, 0.3888889, 0.13636364, 0.18644068]
