[2024-11-04 12:56:03,704][test_4pLoss.py][line:35][INFO] ---------------args---------------
Namespace(data_path='./data/Pymatgen_Wrapped/0', batch_size=128, model_path='/data/ylh/MyExps/MOFV2/checkpoints/Hope_4pLoss/Hope_4pLoss_epoch_194.pth', device='0', mode='test', top_k=5, parallel_model=False, test_name='Hope_4pLoss', num_workers=20, balanced=False, log_name='log/test/test_Hope_4pLoss_2024_11_04_12:55:57.log')
[2024-11-04 12:56:03,706][test_4pLoss.py][line:36][INFO] ---------------model---------------
HopeV1_4pLoss(
  (conv_module): ResTcn(
    (intensity_norm): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (TCN): Sequential(
      (0): ResBlock1D(
        (pre): Conv1d(2, 32, kernel_size=(1,), stride=(1,), bias=False)
        (conv): Sequential(
          (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (2): ResBlock1D(
        (pre): Identity()
        (conv): Sequential(
          (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (4): ResBlock1D(
        (pre): Conv1d(32, 64, kernel_size=(1,), stride=(1,), bias=False)
        (conv): Sequential(
          (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (6): ResBlock1D(
        (pre): Conv1d(64, 128, kernel_size=(1,), stride=(1,), bias=False)
        (conv): Sequential(
          (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (7): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (8): ResBlock1D(
        (pre): Identity()
        (conv): Sequential(
          (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (9): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
      (10): ResBlock1D(
        (pre): Conv1d(128, 256, kernel_size=(1,), stride=(1,), bias=False)
        (conv): Sequential(
          (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (11): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
      (12): ResBlock1D(
        (pre): Identity()
        (conv): Sequential(
          (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (13): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
      (14): ResBlock1D(
        (pre): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
        (conv): Sequential(
          (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (15): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
      (16): ResBlock1D(
        (pre): Identity()
        (conv): Sequential(
          (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (17): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
      (18): ResBlock1D(
        (pre): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
        (conv): Sequential(
          (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (19): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
      (20): Dropout(p=0.25, inplace=False)
      (21): ResBlock1D(
        (pre): Identity()
        (conv): Sequential(
          (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (22): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
      (23): Dropout(p=0.25, inplace=False)
      (24): ResBlock1D(
        (pre): Identity()
        (conv): Sequential(
          (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (25): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
      (26): Dropout(p=0.25, inplace=False)
      (27): ResBlock1D(
        (pre): Identity()
        (conv): Sequential(
          (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (28): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
      (29): Flatten(start_dim=1, end_dim=-1)
    )
  )
  (att): AttentionModule(
    (embed): Embedding(850, 15)
    (patch_conv): PatchConvModule(
      (conv): Sequential(
        (0): Conv2d(1, 2, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        (1): AvgPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0)
        (2): Conv2d(2, 4, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        (3): AvgPool2d(kernel_size=(1, 2), stride=(1, 2), padding=(0, 1))
        (4): Conv2d(4, 8, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        (5): AvgPool2d(kernel_size=(1, 2), stride=(1, 2), padding=(0, 1))
        (6): Conv2d(8, 16, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        (7): AvgPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0)
      )
    )
    (att): TransformerEncoder(
      (positionEnbeding): PositionEmbedding()
      (encoder_layers): ModuleList(
        (0-7): 8 x EncoderLayer(
          (mha): MultiHeadAttention(
            (WQ): Linear(in_features=32, out_features=32, bias=True)
            (WK): Linear(in_features=32, out_features=32, bias=True)
            (WV): Linear(in_features=32, out_features=32, bias=True)
            (scaled_dot_product_attn): ScaledDotProductAttention()
            (linear): Linear(in_features=32, out_features=32, bias=True)
          )
          (dropout1): Dropout(p=0.25, inplace=False)
          (layernorm1): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (linear1): Linear(in_features=32, out_features=256, bias=True)
            (linear2): Linear(in_features=256, out_features=32, bias=True)
            (relu): LeakyReLU(negative_slope=0.01)
          )
          (dropout2): Dropout(p=0.25, inplace=False)
          (layernorm2): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
  )
  (cls_sp): Sequential(
    (0): Linear(in_features=1056, out_features=230, bias=True)
  )
  (cls_cs): Sequential(
    (0): Linear(in_features=1056, out_features=7, bias=True)
  )
  (cls_lt): Sequential(
    (0): Linear(in_features=1056, out_features=6, bias=True)
  )
  (cls_pg): Sequential(
    (0): Linear(in_features=1056, out_features=32, bias=True)
  )
)
[2024-11-04 12:56:03,707][test_4pLoss.py][line:37][INFO] ---------------device---------------
cuda:0
[2024-11-04 12:56:03,707][test_4pLoss.py][line:38][INFO] ---------------lossfn---------------
CrossEntropyLoss()
[2024-11-04 13:02:08,535][test_4pLoss.py][line:93][INFO] ---------------performance---------------
total_num:142729
error:1.749514
total_acc:0.7882981300354004
f1_score:0.5676630735397339
top5_acc:0.9604565501213074
head_acc:0.8016966381243329
medium_acc:0.6010027424092563
tail_add:0.44236587962746426
crystal_system_acc:0.9269594550132751
lattice_type_acc:0.9359415173530579
point_group_acc:0.8626067638397217

[2024-11-04 13:02:08,536][test_4pLoss.py][line:107][INFO] ---------------per_class_acc---------------
[0.743467, 0.7770829, 0.10526316, 0.8706543, 0.82499444, 0.22222222, 0.64776, 0.3693182, 0.7773236, 0.37704918, 0.7514919, 0.6681416, 0.61723846, 0.81423724, 0.77455974, 0.65217394, 0.36842105, 0.7723464, 0.9517478, 0.75164115, 0.46875, 0.21052632, 0.6535433, 0.29411766, 0.0, 0.38157895, 0.11764706, 0.11111111, 0.79820085, 0.30985916, 0.6557971, 0.2777778, 0.83542764, 0.4516129, 0.2, 0.69452447, 0.30508474, 0.59090906, 0.0, 0.3265306, 0.0, 0.57894737, 0.838941, 0.5135135, 0.61022365, 0.4861111, 0.6, 0.3783784, 0.4, 0.2962963, 0.46153846, 0.60575294, 0.5070422, 0.4296875, 0.39072847, 0.64637375, 0.56740445, 0.58474576, 0.45864663, 0.74527675, 0.95200086, 0.84057695, 0.6735967, 0.0, 0.61165047, 0.25757575, 0.0, 0.0, 0.52380955, 0.7346939, 0.6111111, 0.6465116, 0.5923567, 0.67832166, 0.2413793, 0.53601694, 0.29032257, 0.43705463, 0.24626866, 0.5905512, 0.6637931, 0.8484419, 0.125, 0.6041667, 0.8151448, 0.8678679, 0.7036011, 0.9362924, 0.0, 0.34615386, 0.22222222, 0.49591002, 0.0, 0.5377358, 0.40425533, 0.5280172, 0.3125, 0.78431374, 0.0, 0.0, 0.0, 0.15384616, 0.23529412, 0.75409836, 1.0, 0.6037736, 0.16666667, 0.36842105, 0.2173913, 0.8309859, 0.0, 0.23529412, 0.7266187, 0.9248, 0.0, 0.1904762, 0.6363636, 0.71276593, 0.375, 0.53571427, 0.4827586, 0.773913, 0.82051283, 0.82978725, 0.93333334, 0.71428573, 0.47619048, 0.7582418, 0.7303371, 0.89837396, 0.82857144, 0.3, 0.28, 0.75, 0.54, 0.76086956, 0.61403507, 0.6849315, 0.5217391, 0.5833333, 0.6507937, 0.8168498, 0.54385966, 0.48195878, 0.38601035, 0.7709497, 0.76661265, 0.94238126, 0.33333334, 0.24074075, 0.0, 0.49482402, 0.09090909, 0.38481677, 0.36144578, 0.0, 0.11111111, 0.34615386, 0.6486486, 0.53125, 0.88372093, 0.62068963, 0.7242798, 0.46938777, 0.84675324, 0.76038337, 0.90570176, 0.07692308, 0.3971831, 0.46397695, 0.1, 0.06451613, 0.72677594, 0.0625, 0.45833334, 0.7839196, 0.73333335, 0.585034, 0.408, 0.4, 0.36, 0.53608245, 0.0, 0.14285715, 0.21052632, 0.33333334, 0.6818182, 0.25, 0.3181818, 0.4431818, 0.6818182, 0.7627119, 0.57894737, 0.76243097, 0.2, 0.8965517, 0.6666667, 0.9691781, 0.80597013, 0.3846154, 0.68, 0.33333334, 0.8, 0.59375, 0.993007, 0.92957747, 0.53846157, 0.33333334, 0.4, 0.7741935, 0.7241379, 0.5416667, 0.6363636, 0.85714287, 0.5, 0.13636364, 0.87116563, 0.943662, 0.8333333, 0.9931507, 0.9055118, 1.0, 0.84210527, 0.8125, 0.9647059, 0.88095236, 0.8863636, 0.9861111, 0.59090906, 1.0]
