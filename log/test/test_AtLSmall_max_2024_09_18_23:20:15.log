[2024-09-18 23:20:28,416][test.py][line:34][INFO] ---------------args---------------
Namespace(data_path='./data/Pymatgen_Wrapped/2', batch_size=128, model_path='/data/ylh/MyExps/MOFV2/checkpoints/AtLSmall_max/AtLSmall_max_epoch_36.pth', device='2', mode='test', top_k=5, parallel_model=False, test_name='AtLSmall_max', num_workers=20, log_name='log/test/test_AtLSmall_max_2024_09_18_23:20:15.log')
[2024-09-18 23:20:28,431][test.py][line:35][INFO] ---------------model---------------
AtLSmall(
  (embed): Embedding(8501, 15, padding_idx=0)
  (att): TransformerEncoder(
    (positionEnbeding): PositionEmbedding()
    (encoder_layers): ModuleList(
      (0-7): 8 x EncoderLayer(
        (mha): MultiHeadAttention(
          (WQ): Linear(in_features=160, out_features=160, bias=True)
          (WK): Linear(in_features=160, out_features=160, bias=True)
          (WV): Linear(in_features=160, out_features=160, bias=True)
          (scaled_dot_product_attn): ScaledDotProductAttention()
          (linear): Linear(in_features=160, out_features=160, bias=True)
        )
        (dropout1): Dropout(p=0, inplace=False)
        (layernorm1): LayerNorm((160,), eps=1e-06, elementwise_affine=True)
        (ffn): FeedForwardNetwork(
          (linear1): Linear(in_features=160, out_features=256, bias=True)
          (linear2): Linear(in_features=256, out_features=160, bias=True)
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (dropout2): Dropout(p=0, inplace=False)
        (layernorm2): LayerNorm((160,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (cls): Sequential(
    (0): Linear(in_features=160, out_features=230, bias=True)
  )
)
[2024-09-18 23:20:28,431][test.py][line:36][INFO] ---------------device---------------
cuda:2
[2024-09-18 23:20:28,431][test.py][line:37][INFO] ---------------lossfn---------------
CrossEntropyLoss()
[2024-09-18 23:26:34,417][test.py][line:78][INFO] ---------------performance---------------
total_num:141939
error:4.056289
total_acc:0.2951831519603729
f1_score:0.14811931550502777
top5_acc:0.6776432394981384
head_acc:0.3280773166759221
medium_acc:0.17664386108919602
tail_add:0.13376940517301975

[2024-09-18 23:26:34,418][test.py][line:89][INFO] ---------------per_class_acc---------------
[0.30791017, 0.31847754, 0.0, 0.3398374, 0.25274232, 0.0, 0.15778604, 0.125, 0.33915836, 0.033333335, 0.14193825, 0.09343715, 0.11449943, 0.4218633, 0.27890065, 0.6818182, 0.0, 0.25035295, 0.52682406, 0.12251656, 0.06451613, 0.0, 0.016, 0.0, 0.0, 0.065789476, 0.0, 0.0, 0.13882124, 0.07042254, 0.09818182, 0.022222223, 0.24685514, 0.077419356, 0.2, 0.14697406, 0.0, 0.045454547, 0.0, 0.071428575, 0.0, 0.15789473, 0.19160685, 0.0, 0.025723472, 0.041666668, 0.2, 0.054054055, 0.0, 0.09259259, 0.03846154, 0.12881356, 0.042857144, 0.031496063, 0.04, 0.059669685, 0.022177419, 0.11363637, 0.03787879, 0.3128068, 0.40839204, 0.37061995, 0.13987474, 0.0, 0.067961164, 0.0, 0.0, 0.0, 0.0952381, 0.0703259, 0.18309858, 0.10747664, 0.012820513, 0.08391608, 0.0, 0.17372881, 0.048387095, 0.12589073, 0.02238806, 0.08661418, 0.060344826, 0.30169973, 0.0, 0.0, 0.10244989, 0.14586467, 0.16620499, 0.3307128, 0.0, 0.115384616, 0.0, 0.2717949, 0.0, 0.056603774, 0.04255319, 0.107027024, 0.0, 0.21568628, 0.5, 0.0, 0.0, 0.0, 0.0, 0.08196721, 0.0, 0.0, 0.0, 0.0, 0.04347826, 0.103286386, 0.0, 0.0, 0.21582733, 0.16025642, 0.0, 0.0952381, 0.15151516, 0.10638298, 0.0, 0.0, 0.11494253, 0.072674416, 0.14102565, 0.31914893, 0.36666667, 0.071428575, 0.04761905, 0.18681319, 0.19101124, 0.18775511, 0.31428573, 0.0, 0.0, 0.071428575, 0.06, 0.054945055, 0.0, 0.19178082, 0.069565214, 0.15277778, 0.136, 0.08791209, 0.21052632, 0.185567, 0.24352331, 0.30209792, 0.2857143, 0.5707526, 0.16666667, 0.074074075, 0.0, 0.10559006, 0.0, 0.060686015, 0.13253012, 0.0, 0.0, 0.09615385, 0.17837837, 0.140625, 0.13372093, 0.10344828, 0.33744857, 0.1632653, 0.18701299, 0.3301282, 0.2777168, 0.0, 0.11299435, 0.19020173, 0.05, 0.0, 0.087671235, 0.0625, 0.0, 0.19143577, 0.0, 0.020408163, 0.024, 0.025, 0.0, 0.26804122, 0.0, 0.0, 0.05263158, 0.14583333, 0.45454547, 0.0, 0.27272728, 0.0, 0.22727273, 0.5084746, 0.05263158, 0.31111112, 0.1, 0.22413793, 0.25, 0.6369863, 0.67164177, 0.0, 0.04, 0.041666668, 0.16363636, 0.328125, 0.4289044, 0.22535211, 0.23076923, 0.0, 0.057142857, 0.2580645, 0.41379312, 0.083333336, 0.15151516, 0.0, 0.0, 0.045454547, 0.6196319, 0.1971831, 0.14583333, 0.23972602, 0.35433072, 0.5, 0.13157895, 0.125, 0.7382353, 0.4047619, 0.27272728, 0.4027778, 0.09090909, 0.22033899]
