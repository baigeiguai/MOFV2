[2024-10-15 10:41:31,707][test.py][line:34][INFO] ---------------args---------------
Namespace(data_path='./data/Pymatgen_Wrapped/0', batch_size=128, model_path='/data/ylh/MyExps/MOFV2/checkpoints/HopeV1_2Loss/HopeV1_2Loss_epoch_146.pth', device='3', mode='test', top_k=5, parallel_model=False, test_name='HopeV1_2Loss', num_workers=20, log_name='log/test/test_HopeV1_2Loss_2024_10_15_10:41:23.log')
[2024-10-15 10:41:31,709][test.py][line:35][INFO] ---------------model---------------
HopeV1(
  (conv_module): ResTcn(
    (intensity_norm): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (TCN): Sequential(
      (0): ResBlock1D(
        (pre): Conv1d(2, 32, kernel_size=(1,), stride=(1,), bias=False)
        (conv): Sequential(
          (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (2): ResBlock1D(
        (pre): Identity()
        (conv): Sequential(
          (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (4): ResBlock1D(
        (pre): Conv1d(32, 64, kernel_size=(1,), stride=(1,), bias=False)
        (conv): Sequential(
          (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (6): ResBlock1D(
        (pre): Conv1d(64, 128, kernel_size=(1,), stride=(1,), bias=False)
        (conv): Sequential(
          (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (7): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (8): ResBlock1D(
        (pre): Identity()
        (conv): Sequential(
          (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (9): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
      (10): ResBlock1D(
        (pre): Conv1d(128, 256, kernel_size=(1,), stride=(1,), bias=False)
        (conv): Sequential(
          (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (11): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
      (12): ResBlock1D(
        (pre): Identity()
        (conv): Sequential(
          (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (13): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
      (14): ResBlock1D(
        (pre): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
        (conv): Sequential(
          (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (15): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(1,))
      (16): ResBlock1D(
        (pre): Identity()
        (conv): Sequential(
          (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (17): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
      (18): ResBlock1D(
        (pre): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
        (conv): Sequential(
          (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (19): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
      (20): Dropout(p=0.2, inplace=False)
      (21): ResBlock1D(
        (pre): Identity()
        (conv): Sequential(
          (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (22): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
      (23): Dropout(p=0.2, inplace=False)
      (24): ResBlock1D(
        (pre): Identity()
        (conv): Sequential(
          (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (25): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
      (26): Dropout(p=0.2, inplace=False)
      (27): ResBlock1D(
        (pre): Identity()
        (conv): Sequential(
          (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): LeakyReLU(negative_slope=0.01)
          (3): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
          (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (28): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
      (29): Flatten(start_dim=1, end_dim=-1)
    )
  )
  (att): AttentionModule(
    (embed): Embedding(850, 15)
    (patch_conv): PatchConvModule(
      (conv): Sequential(
        (0): Conv2d(1, 2, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        (1): AvgPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0)
        (2): Conv2d(2, 4, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        (3): AvgPool2d(kernel_size=(1, 2), stride=(1, 2), padding=(0, 1))
        (4): Conv2d(4, 8, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        (5): AvgPool2d(kernel_size=(1, 2), stride=(1, 2), padding=(0, 1))
        (6): Conv2d(8, 16, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        (7): AvgPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0)
      )
    )
    (att): TransformerEncoder(
      (positionEnbeding): PositionEmbedding()
      (encoder_layers): ModuleList(
        (0-7): 8 x EncoderLayer(
          (mha): MultiHeadAttention(
            (WQ): Linear(in_features=32, out_features=32, bias=True)
            (WK): Linear(in_features=32, out_features=32, bias=True)
            (WV): Linear(in_features=32, out_features=32, bias=True)
            (scaled_dot_product_attn): ScaledDotProductAttention()
            (linear): Linear(in_features=32, out_features=32, bias=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (layernorm1): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (linear1): Linear(in_features=32, out_features=256, bias=True)
            (linear2): Linear(in_features=256, out_features=32, bias=True)
            (relu): LeakyReLU(negative_slope=0.01)
          )
          (dropout2): Dropout(p=0.2, inplace=False)
          (layernorm2): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
  )
  (cls_sp): Sequential(
    (0): Linear(in_features=1056, out_features=512, bias=True)
    (1): Linear(in_features=512, out_features=230, bias=True)
  )
  (cls_cs): Sequential(
    (0): Linear(in_features=1056, out_features=256, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=256, out_features=64, bias=True)
    (3): LeakyReLU(negative_slope=0.01)
    (4): Linear(in_features=64, out_features=7, bias=True)
  )
)
[2024-10-15 10:41:31,710][test.py][line:36][INFO] ---------------device---------------
cuda:3
[2024-10-15 10:41:31,710][test.py][line:37][INFO] ---------------lossfn---------------
CrossEntropyLoss()
[2024-10-15 10:44:16,340][test.py][line:78][INFO] ---------------performance---------------
total_num:142729
error:1.801690
total_acc:0.7808504104614258
f1_score:0.5472683906555176
top5_acc:0.9587469696998596
head_acc:0.8355573442878209
medium_acc:0.7621083285238891
tail_add:0.5976091483824745

[2024-10-15 10:44:16,340][test.py][line:89][INFO] ---------------per_class_acc---------------
[0.7604229, 0.76287895, 0.078947365, 0.86234075, 0.8140983, 0.2777778, 0.6720099, 0.36363637, 0.80007106, 0.36065573, 0.7242114, 0.62942475, 0.62120783, 0.84044135, 0.7340335, 0.65217394, 0.42105263, 0.76582867, 0.9368432, 0.7582057, 0.25, 0.10526316, 0.70866144, 0.2647059, 0.0, 0.47368422, 0.23529412, 0.0, 0.8018478, 0.30985916, 0.67391306, 0.3, 0.84029186, 0.43225807, 0.2, 0.67146975, 0.3220339, 0.6363636, 0.0, 0.3469388, 0.0, 0.5, 0.8240485, 0.35135135, 0.55271566, 0.45833334, 0.53333336, 0.2972973, 0.4, 0.3148148, 0.30769232, 0.5651438, 0.5492958, 0.390625, 0.30463576, 0.63472736, 0.49295774, 0.59887004, 0.35338345, 0.7443876, 0.958493, 0.7951923, 0.66943866, 0.0, 0.776699, 0.21212122, 0.0, 0.0, 0.50793654, 0.67517006, 0.5138889, 0.7116279, 0.38853502, 0.6503497, 0.1724138, 0.4088983, 0.24193548, 0.543943, 0.24626866, 0.6062992, 0.63793105, 0.83994335, 0.0625, 0.5833333, 0.7906459, 0.8678679, 0.5761773, 0.94308096, 0.0, 0.48076922, 0.13333334, 0.4243354, 0.0, 0.5188679, 0.31914893, 0.6002155, 0.39583334, 0.74509805, 0.0, 0.0, 0.0, 0.23076923, 0.3529412, 0.704918, 1.0, 0.5471698, 0.16666667, 0.31578946, 0.26086956, 0.82159626, 0.0, 0.1764706, 0.705036, 0.888, 0.0, 0.2857143, 0.6666667, 0.62765956, 0.4375, 0.5714286, 0.4827586, 0.7246377, 0.8333333, 0.82978725, 0.93333334, 0.6875, 0.5714286, 0.74725276, 0.8651685, 0.8780488, 0.8428571, 0.2, 0.32, 0.71428573, 0.54, 0.6304348, 0.5614035, 0.7123288, 0.6347826, 0.625, 0.61904764, 0.7692308, 0.5350877, 0.4072165, 0.48704663, 0.75418997, 0.73257697, 0.92595094, 0.33333334, 0.24074075, 0.0, 0.5362319, 0.09090909, 0.30890054, 0.34136546, 0.0, 0.22222222, 0.3653846, 0.7081081, 0.578125, 0.8875969, 0.6896552, 0.74897116, 0.53061223, 0.82857144, 0.71246004, 0.8969298, 0.07692308, 0.43098593, 0.4610951, 0.175, 0.06451613, 0.647541, 0.0625, 0.375, 0.7663317, 0.8, 0.6666667, 0.256, 0.225, 0.32, 0.48453608, 0.0, 0.2857143, 0.21052632, 0.375, 0.6818182, 0.375, 0.27272728, 0.375, 0.6515151, 0.8305085, 0.57894737, 0.7016575, 0.2, 0.86206895, 0.61904764, 0.9691781, 0.73134327, 0.23076923, 0.56, 0.16666667, 0.8363636, 0.578125, 0.997669, 0.91549295, 0.46153846, 0.0, 0.4, 0.7419355, 0.6896552, 0.6666667, 0.54545456, 0.85714287, 0.5, 0.13636364, 0.78527606, 0.92957747, 0.8541667, 1.0, 0.9212598, 0.98717946, 0.81578946, 0.75, 0.9529412, 0.7619048, 0.89772725, 1.0, 0.5, 0.9830508]
