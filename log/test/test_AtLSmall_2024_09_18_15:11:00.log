[2024-09-18 15:11:09,264][test.py][line:34][INFO] ---------------args---------------
Namespace(data_path='./data/Pymatgen_Wrapped/2', batch_size=128, model_path='/data/ylh/MyExps/MOFV2/checkpoints/AtLSmall/AtLSmall_epoch_20.pth', device='1', mode='test', top_k=5, parallel_model=False, test_name='AtLSmall', num_workers=20, log_name='log/test/test_AtLSmall_2024_09_18_15:11:00.log')
[2024-09-18 15:11:09,266][test.py][line:35][INFO] ---------------model---------------
AtLSmall(
  (embed): Embedding(8501, 15, padding_idx=0)
  (att): TransformerEncoder(
    (positionEnbeding): PositionEmbedding()
    (encoder_layers): ModuleList(
      (0-7): 8 x EncoderLayer(
        (mha): MultiHeadAttention(
          (WQ): Linear(in_features=160, out_features=160, bias=True)
          (WK): Linear(in_features=160, out_features=160, bias=True)
          (WV): Linear(in_features=160, out_features=160, bias=True)
          (scaled_dot_product_attn): ScaledDotProductAttention()
          (linear): Linear(in_features=160, out_features=160, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (layernorm1): LayerNorm((160,), eps=1e-06, elementwise_affine=True)
        (ffn): FeedForwardNetwork(
          (linear1): Linear(in_features=160, out_features=256, bias=True)
          (linear2): Linear(in_features=256, out_features=160, bias=True)
          (relu): LeakyReLU(negative_slope=0.01)
        )
        (dropout2): Dropout(p=0.1, inplace=False)
        (layernorm2): LayerNorm((160,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (cls): Sequential(
    (0): Linear(in_features=160, out_features=200, bias=True)
    (1): Linear(in_features=200, out_features=230, bias=True)
  )
)
[2024-09-18 15:11:09,266][test.py][line:36][INFO] ---------------device---------------
cuda:1
[2024-09-18 15:11:09,266][test.py][line:37][INFO] ---------------lossfn---------------
CrossEntropyLoss()
[2024-09-18 15:15:27,839][test.py][line:78][INFO] ---------------performance---------------
total_num:141939
error:2.609745
total_acc:0.34349966049194336
f1_score:0.14998209476470947
top5_acc:0.733639121055603
head_acc:0.385788689140658
medium_acc:0.19458568300967194
tail_add:0.12467017262084396

[2024-09-18 15:15:27,840][test.py][line:89][INFO] ---------------per_class_acc---------------
[0.528424, 0.51236176, 0.0, 0.49647698, 0.32930377, 0.0, 0.06732755, 0.056818184, 0.3471826, 0.0, 0.1406518, 0.12013348, 0.0863061, 0.35650873, 0.26692334, 0.6818182, 0.0, 0.3204706, 0.6553648, 0.09602649, 0.0, 0.0, 0.016, 0.0, 0.0, 0.05263158, 0.0, 0.0, 0.04359474, 0.028169014, 0.061818182, 0.0, 0.26146355, 0.103225805, 0.2, 0.16714698, 0.0, 0.0, 0.0, 0.040816326, 0.0, 0.10526316, 0.23799007, 0.0, 0.019292604, 0.11111111, 0.06666667, 0.0, 0.0, 0.074074075, 0.0, 0.096610166, 0.042857144, 0.0, 0.0, 0.018114012, 0.012096774, 0.09659091, 0.022727273, 0.24854976, 0.55242366, 0.42606854, 0.11691023, 0.0, 0.0776699, 0.0, 0.0, 0.0, 0.015873017, 0.135506, 0.056338027, 0.07476635, 0.0, 0.11888112, 0.0, 0.11016949, 0.016129032, 0.14489311, 0.03731343, 0.03937008, 0.060344826, 0.34560907, 0.0625, 0.0, 0.111358576, 0.16390978, 0.1966759, 0.3768344, 0.0, 0.13461539, 0.0, 0.22871795, 0.0, 0.06603774, 0.0, 0.18378378, 0.0625, 0.4117647, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09836066, 0.0, 0.0, 0.0, 0.05263158, 0.04347826, 0.08450704, 0.0, 0.0, 0.22302158, 0.13301282, 0.0, 0.0952381, 0.09090909, 0.04255319, 0.0, 0.017857144, 0.18390805, 0.075581394, 0.08974359, 0.34042552, 0.43333334, 0.09821428, 0.0, 0.0989011, 0.11235955, 0.18367347, 0.31428573, 0.0, 0.041666668, 0.035714287, 0.0, 0.14285715, 0.03508772, 0.1780822, 0.09565217, 0.06944445, 0.2, 0.1941392, 0.15789473, 0.36597937, 0.06735751, 0.42097902, 0.2857143, 0.63677335, 0.33333334, 0.018518519, 0.0, 0.12836438, 0.0, 0.039577838, 0.08835341, 0.0, 0.11111111, 0.07692308, 0.14054054, 0.2421875, 0.2364341, 0.03448276, 0.25514403, 0.1632653, 0.33506495, 0.39423078, 0.27442372, 0.0, 0.22881356, 0.09221902, 0.0, 0.0, 0.073972605, 0.0, 0.041666668, 0.14105794, 0.071428575, 0.006802721, 0.048, 0.025, 0.0, 0.26804122, 0.0, 0.0, 0.05263158, 0.041666668, 0.27272728, 0.0, 0.0, 0.0, 0.27272728, 0.6101695, 0.02631579, 0.23333333, 0.0, 0.41379312, 0.44047618, 0.739726, 0.41791046, 0.0, 0.04, 0.0, 0.12727273, 0.359375, 0.72494173, 0.32394367, 0.15384616, 0.0, 0.028571429, 0.2580645, 0.27586207, 0.125, 0.24242425, 0.071428575, 0.0, 0.045454547, 0.63190186, 0.33802816, 0.083333336, 0.42465752, 0.52755904, 0.55128205, 0.10526316, 0.125, 0.7323529, 0.2857143, 0.22727273, 0.3888889, 0.18181819, 0.15254237]
